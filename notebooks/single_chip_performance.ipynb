{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHPGYEr0fvoR"
      },
      "source": [
        "# Measuring Single Chip Performance on Tensor Processing Tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MICtvit6fvoR",
        "outputId": "c5d92777-d88d-4547-93db-107e0aa7a950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on darwin\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "import jax\n",
        "\n",
        "platform : Literal[\"darwin\", \"colab\", \"cuda\"] = \"darwin\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    platform = \"colab\"\n",
        "except ImportError:\n",
        "    devices = jax.devices()\n",
        "    if any(d.platform == \"gpu\" for d in devices):\n",
        "        platform = \"cuda\"\n",
        "\n",
        "print(f\"Running on {platform}\")\n",
        "\n",
        "if platform == \"colab\":\n",
        "    !git clone https://github.com/novastar53/high_performance_jax\n",
        "    !cd high_performance_jax && git pull\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    !cd deepkit && git pull\n",
        "    hpj_dir = str(Path().absolute() / \"high_performance_jax\" / \"src\" )\n",
        "    dt_dir = str(Path().absolute() / \"deepkit\" / \"src\" )\n",
        "    sys.path.append(hpj_dir)\n",
        "    print(hpj_dir)\n",
        "    sys.path.append(dt_dir)\n",
        "    print(dt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "68axfBF8fvoS",
        "outputId": "5810b23c-55fe-44f7-89ef-fb5eaed99ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Devices:\n",
            "1. cpu\n",
            "dim 4 | average time (ms): 0.01 | hbm xfer/s 6.49e+06 | flops 1.12e+02 | intensity 1.1667 |teraflops/s 0.0000\n",
            "dim 8 | average time (ms): 0.01 | hbm xfer/s 2.95e+07 | flops 9.60e+02 | intensity 2.5000 |teraflops/s 0.0001\n",
            "dim 16 | average time (ms): 0.02 | hbm xfer/s 7.64e+07 | flops 7.94e+03 | intensity 5.1667 |teraflops/s 0.0004\n",
            "dim 32 | average time (ms): 0.02 | hbm xfer/s 4.04e+08 | flops 6.45e+04 | intensity 10.5000 |teraflops/s 0.0042\n",
            "dim 64 | average time (ms): 0.03 | hbm xfer/s 8.81e+08 | flops 5.20e+05 | intensity 21.1667 |teraflops/s 0.0186\n",
            "dim 128 | average time (ms): 0.07 | hbm xfer/s 1.32e+09 | flops 4.18e+06 | intensity 42.5000 |teraflops/s 0.0561\n",
            "dim 256 | average time (ms): 0.20 | hbm xfer/s 1.95e+09 | flops 3.35e+07 | intensity 85.1667 |teraflops/s 0.1662\n",
            "dim 512 | average time (ms): 0.87 | hbm xfer/s 1.81e+09 | flops 2.68e+08 | intensity 170.5000 |teraflops/s 0.3079\n",
            "dim 1024 | average time (ms): 4.28 | hbm xfer/s 1.47e+09 | flops 2.15e+09 | intensity 341.1667 |teraflops/s 0.5020\n",
            "dim 2048 | average time (ms): 29.35 | hbm xfer/s 8.57e+08 | flops 1.72e+10 | intensity 682.5000 |teraflops/s 0.5852\n",
            "dim 4096 | average time (ms): 224.66 | hbm xfer/s 4.48e+08 | flops 1.37e+11 | intensity 1,365.1667 |teraflops/s 0.6117\n",
            "dim 8192 | average time (ms): 1658.67 | hbm xfer/s 2.43e+08 | flops 1.10e+12 | intensity 2,730.5000 |teraflops/s 0.6628\n"
          ]
        }
      ],
      "source": [
        "from deepkit.utils import timeit\n",
        "from high_performance_jax.single_chip_performance import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dtype = jnp.bfloat16\n",
        "devices = jax.devices()\n",
        "print(\"Devices:\")\n",
        "for i,d in enumerate(devices):\n",
        "  print(f\"{i+1}. {d.device_kind}\")    # e.g. “TPU v3”\n",
        "\n",
        "\n",
        "#####################################\n",
        "##        jax.lax matmul presets   ##\n",
        "#####################################\n",
        "## 'ANY_F8_ANY_F8_F32',\n",
        "## 'ANY_F8_ANY_F8_F32_FAST_ACCUM'\n",
        "## 'ANY_F8_ANY_F8_ANY'\n",
        "## 'ANY_F8_ANY_F8_ANY_FAST_ACCUM'\n",
        "## 'F16_F16_F16'\n",
        "## 'F16_F16_F32'\n",
        "## 'BF16_BF16_BF16'\n",
        "## 'BF16_BF16_F32'\n",
        "## 'BF16_BF16_F32_X3'\n",
        "## 'BF16_BF16_F32_X6'\n",
        "## 'TF32_TF32_F32'\n",
        "## 'TF32_TF32_F32_X3'\n",
        "## 'F32_F32_F32'\n",
        "## 'F64_F64_F64'\n",
        "#####################################\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"BF16_BF16_F32\") # Set the default precision for matrix multiplication\n",
        "\n",
        "dims = [2**i for i in range(2, 15)]\n",
        "times = []\n",
        "tflops = []\n",
        "hbm_rates = []\n",
        "\n",
        "for dim in dims:\n",
        "    A = jnp.ones((dim, dim), dtype=dtype)\n",
        "    B = jnp.ones((dim, dim), dtype=dtype)\n",
        "    #C = jnp.ones((dim, dim), dtype=dtype)\n",
        "    task = \"matmul\"\n",
        "\n",
        "    average_time_ms = timeit(jax.jit(matmul), A, B)\n",
        "    flops = measure_tpu_flops(task, dim)\n",
        "    times.append(average_time_ms)\n",
        "    tflops.append(1000 * flops / average_time_ms / 10**12)\n",
        "    hbm_xfer = measure_tpu_hbm_memory_transfer(task, dim, dtype)\n",
        "    hbm_rates.append(1000 * hbm_xfer / average_time_ms)\n",
        "    arithmetic_intensity = flops / hbm_xfer\n",
        "\n",
        "    print(f\"dim {dim} | average time (ms): {average_time_ms:.2f} | \"\n",
        "            f\"hbm xfer/s {hbm_rates[-1]:.2e} | \"\n",
        "            f\"flops {flops:.2e} | \"\n",
        "            f\"intensity {arithmetic_intensity:,.4f} |\"\n",
        "            f\"teraflops/s {tflops[-1]:,.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ax1.plot(dims, times, marker='o')\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel(\"Matrix Dimension (dim)\")\n",
        "ax1.set_ylabel(\"Average Time (ms)\")\n",
        "ax1.set_title(\"Matrix Multiplication Time vs Dimension\")\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(dims, tflops, marker='o')\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_xlabel(\"Matrix Dimension (dim)\")\n",
        "ax2.set_ylabel(\"TFLOPS/s\")\n",
        "ax2.set_title(\"Performance in TFLOPS/s vs Dimension\")\n",
        "ax2.grid(True)\n",
        "\n",
        "ax3.plot(dims, hbm_rates, marker='o')\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel(\"Matrix Dimension (dim)\")\n",
        "ax3.set_ylabel(\"HBM Transfer Rate (bytes/s)\")\n",
        "ax3.set_title(\"HBM Transfer Rate vs Dimension\")\n",
        "ax3.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
