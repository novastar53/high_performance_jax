{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHPGYEr0fvoR"
      },
      "source": [
        "# Measuring Single Chip Performance on Tensor Processing Tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MICtvit6fvoR",
        "outputId": "ec8545ed-fb2f-481a-b2f2-6ff9756ab140"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "import jax\n",
        "\n",
        "platform : Literal[\"darwin\", \"colab\", \"cuda\"] = \"darwin\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    platform = \"colab\"\n",
        "except ImportError:\n",
        "    devices = jax.devices()\n",
        "    if any(d.platform == \"gpu\" for d in devices):\n",
        "        platform = \"cuda\"\n",
        "\n",
        "print(f\"Running on {platform}\")\n",
        "\n",
        "if platform == \"colab\":\n",
        "    !git clone https://github.com/novastar53/high_performance_jax\n",
        "    !cd high_performance_jax && git pull\n",
        "    !git clone https://github.com/novastar53/deepkit\n",
        "    !cd deepkit && git pull\n",
        "    hpj_dir = str(Path().absolute() / \"high_performance_jax\" / \"src\" )\n",
        "    dt_dir = str(Path().absolute() / \"deepkit\" / \"src\" )\n",
        "    sys.path.append(hpj_dir)\n",
        "    print(hpj_dir)\n",
        "    sys.path.append(dt_dir)\n",
        "    print(dt_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "68axfBF8fvoS",
        "outputId": "5c44302d-44f5-47e2-d6a9-e83621477207"
      },
      "outputs": [],
      "source": [
        "from deepkit.utils import timeit\n",
        "from high_performance_jax.single_chip_performance import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dtype = jnp.bfloat16\n",
        "devices = jax.devices()\n",
        "print(\"Devices:\")\n",
        "for i,d in enumerate(devices):\n",
        "  print(f\"{i+1}. {d.device_kind}\")    # e.g. “TPU v3”\n",
        "\n",
        "\n",
        "#####################################\n",
        "##        jax.lax matmul presets   ##\n",
        "#####################################\n",
        "## 'ANY_F8_ANY_F8_F32',\n",
        "## 'ANY_F8_ANY_F8_F32_FAST_ACCUM'\n",
        "## 'ANY_F8_ANY_F8_ANY'\n",
        "## 'ANY_F8_ANY_F8_ANY_FAST_ACCUM'\n",
        "## 'F16_F16_F16'\n",
        "## 'F16_F16_F32'\n",
        "## 'BF16_BF16_BF16'\n",
        "## 'BF16_BF16_F32'\n",
        "## 'BF16_BF16_F32_X3'\n",
        "## 'BF16_BF16_F32_X6'\n",
        "## 'TF32_TF32_F32'\n",
        "## 'TF32_TF32_F32_X3'\n",
        "## 'F32_F32_F32'\n",
        "## 'F64_F64_F64'\n",
        "#####################################\n",
        "jax.config.update(\"jax_default_matmul_precision\", \"BF16_BF16_F32\") # Set the default precision for matrix multiplication\n",
        "\n",
        "dims = [2**i for i in range(6, 16)]\n",
        "times = []\n",
        "tflops = []\n",
        "hbm_rates = []\n",
        "intensities = []\n",
        "\n",
        "for dim in dims:\n",
        "    n = 2**28 // dim ** 2\n",
        "    A = jnp.ones((n, dim, dim), dtype=dtype)\n",
        "    B = jnp.ones((n, dim, dim), dtype=dtype)\n",
        "    #C = jnp.ones((dim, dim), dtype=dtype)\n",
        "    task = \"matmul\"\n",
        "\n",
        "    average_time_ms = timeit(jax.jit(bmm), A, B)\n",
        "    flops = measure_tpu_flops(task, n, dim)\n",
        "    times.append(average_time_ms)\n",
        "    tflops.append(1000 * flops / average_time_ms / 10**12)\n",
        "    hbm_xfer = measure_tpu_hbm_memory_transfer(task, n, dim, dtype)\n",
        "    hbm_rates.append(1000 * hbm_xfer / average_time_ms)\n",
        "    arithmetic_intensity = flops / hbm_xfer\n",
        "    intensities.append(arithmetic_intensity)\n",
        "\n",
        "    print(f\"dim {dim} | average time (ms): {average_time_ms:.2f} | \"\n",
        "            f\"hbm xfer/s {hbm_rates[-1]:.2e} | \"\n",
        "            f\"flops {flops:.2e} | \"\n",
        "            f\"intensity {arithmetic_intensity:,.4f} |\"\n",
        "            f\"teraflops/s {tflops[-1]:,.4f}\")\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "ax1.plot(dims, times, marker='o')\n",
        "#ax1.set_xscale('log')\n",
        "ax1.set_xlabel(\"Matrix Dimension (dim)\")\n",
        "ax1.set_ylabel(\"Average Time (ms)\")\n",
        "ax1.set_title(\"Matrix Multiplication Time vs Dimension\")\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.plot(intensities, tflops, marker='o')\n",
        "#ax1.set_xscale('log')\n",
        "ax2.set_xlabel(\"Arithmetic Intensity\")\n",
        "ax2.set_ylabel(\"TFLOPS/s\")\n",
        "ax2.set_title(\"TFLOPS/s vs Artithmetic Intensity\")\n",
        "ax2.grid(True)\n",
        "\n",
        "ax3.plot(dims, hbm_rates, marker='o')\n",
        "#ax3.set_xscale('log')\n",
        "ax3.set_xlabel(\"Matrix Dimension (dim)\")\n",
        "ax3.set_ylabel(\"HBM Transfer Rate (bytes/s)\")\n",
        "ax3.set_title(\"HBM Transfer Rate vs Dimension\")\n",
        "ax3.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4NEEFxSRwLy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
