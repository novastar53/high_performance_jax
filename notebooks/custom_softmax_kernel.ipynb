{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a86036",
   "metadata": {},
   "source": [
    "# How to Write a Softmax Kernel in Pallas\n",
    "\n",
    "\n",
    "The softmax function is fundamental to neural network training because it converts raw model outputs (logits) into valid probability distributions. This is essential for classification tasks where we need to interpret network predictions as probabilities over discrete classes. Softmax enables the use of cross-entropy loss, which provides clear learning signals during backpropagation and is numerically efficient. Additionally, softmax is differentiable, allowing gradients to flow effectively through the network during training, which is why it's the standard choice for the output layer in multi-class classification models.\n",
    "\n",
    "In the previous post [Writing a GPU Kernel for Matrix Multiplication], we wrote GPU kernel in Pallas for performing efficient matrix multiplication. In this post, we'll build on this by writing a GPU kernel for the softmax function. We will also write the backward pass and test it with a neural network training step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1062b",
   "metadata": {},
   "source": [
    "## Softmax Operation\n",
    "\n",
    "Given an input vector $z = (z_1, ..., z_n) \\in R^n$, the softmax function $σ : R^n → (0,1)^n$ produces a probability distribution over the n entries:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^n \\exp(z_j)} \\quad\\text{for } i=1,\\dots,n.\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Each output is positive and they sum to $1: ∑_i σ(z)_i = 1$.\n",
    "- Softmax is invariant to shifts: $σ(z) = σ(z + c·1)$ for any scalar c. For numerical stability one commonly uses\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i - \\max_j z_j)}{\\sum_{k=1}^n \\exp(z_k - \\max_j z_j)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a642097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.pallas as pl\n",
    "from jax.experimental.pallas import triton as plgpu\n",
    "\n",
    "\n",
    "def manual_softmax(logits):\n",
    "    m = jnp.max(logits, axis=-1)         # 1\n",
    "    s = jnp.exp(logits - m[..., None])   # 2\n",
    "    l = jnp.sum(s, axis=-1)              # 3\n",
    "    return s / l[..., None]              # 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d3acb1",
   "metadata": {},
   "source": [
    "Now imagine trying to implement this on a GPU. For computing line #1, the entire logits tensor will have to be loaded into the GPU cache. This will be extremely slow for large matrices. What if we break it up into blocks and compute the max for each block independently? Since the max is across columns, it's easy to tile across the rows. However, the column axis might still overflow and cause a bottleneck. \n",
    "Since we cannot parallelize the row operations, we can still chunk it, calculate the max for each block independently, correcting it as we move across blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00557c1",
   "metadata": {},
   "source": [
    "\n",
    "## Online Softmax\n",
    "\n",
    "The key idea is to maintain running estimates of $m$ and $l$ as we iterate through the input block-wise. \n",
    "1. **Track the running maximum**: As we process elements, we keep track of the largest value seen so far.\n",
    "2. **Update the sum of exponentials**: When we encounter a new maximum, we rescale previous exponentials and add the new ones.\n",
    "3. **Compute probabilities**: Finally, divide each exponential by the total sum.\n",
    "\n",
    "This avoids numerical overflow/underflow and allows us to process data in blocks without materializing the full exponential array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f94b8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_M = 64\n",
    "BLOCK_N = 64\n",
    "NUM_WARPS = 4\n",
    "NUM_STAGES = 3\n",
    "\n",
    "# Online softmax\n",
    "def online_softmax(logits):\n",
    "    out = jnp.zeros_like(logits)\n",
    "    m = jnp.full((logits.shape[0],), -jnp.inf)\n",
    "    l = jnp.zeros((logits.shape[0],))\n",
    "    for i in range(0, logits.shape[0], BLOCK_M):  # This axis can be tiled in parallel blocks.\n",
    "        for j in range(0, logits.shape[1], BLOCK_N):  # This axis cannot be tiled in parallel, so it is tiled sequentially\n",
    "            block = logits[i:i+BLOCK_M, j:j+BLOCK_N] # Load a block\n",
    "            block_max = jnp.max(block, axis=-1) # Get the max across the block\n",
    "            curr_max = m[i:i+BLOCK_M] # Retrieve the previous computed max for the rows\n",
    "            new_max = jnp.maximum(curr_max, block_max) # Update the max for all the rows\n",
    "            m = m.at[i:i+BLOCK_M].set(new_max)  \n",
    "            l_block = l[i:i+BLOCK_M] # Get the denominator for the rows in the block\n",
    "            l_block = l_block * jnp.exp(curr_max - new_max) + jnp.sum( # Correct and update the denominator based on the current block\n",
    "                jnp.exp(block - new_max[:, None]), axis=-1\n",
    "            )\n",
    "            l = l.at[i:i+BLOCK_M].set(l_block)\n",
    "        for j in range(0, logits.shape[1], BLOCK_N):  # Loop over the column blocks and generate the output values \n",
    "            out_block = jnp.exp(logits[i:i+BLOCK_M, j:j+BLOCK_N] - m[i:i+BLOCK_M][:, None]) / l[i:i+BLOCK_M][:, None]\n",
    "            out = out.at[i:i+BLOCK_M, j:j+BLOCK_N].set(out_block)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea77d7a",
   "metadata": {},
   "source": [
    "The next step is to convert this into an efficient GPU kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44070d44",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7b046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPRET_MODE = True # Set to False on GPU\n",
    "\n",
    "def softmax_kernel(x_ref, out_ref, *, n_col_blocks, n_rows, n_cols):\n",
    "    max_reg = jnp.full((BLOCK_M,), -jnp.inf, dtype=jnp.float32) \n",
    "    l_reg = jnp.zeros((BLOCK_M,), dtype=jnp.float32) \n",
    "    row_ids = pl.program_id(0) * BLOCK_M + jnp.arange(BLOCK_M)\n",
    "    row_mask = row_ids < n_rows\n",
    "\n",
    "    def stats_body(t, args):\n",
    "        max_reg, l_reg = args\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        max_tile = jnp.max(x_tile, axis=-1)\n",
    "        max_new = jnp.maximum(max_reg, max_tile)\n",
    "        l_update = l_reg * jnp.exp(max_reg - max_new) + jnp.sum(\n",
    "            jnp.exp(x_tile - max_new[:, None]), axis=-1\n",
    "        )\n",
    "        return max_new, l_update\n",
    "        \n",
    "    max_reg, l_reg = jax.lax.fori_loop(0, n_col_blocks, stats_body, (max_reg, l_reg))\n",
    "\n",
    "    def out_body(t, _):\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        out_tile = jnp.exp(x_tile - max_reg[:, None]) / l_reg[:, None]\n",
    "        plgpu.store(out_ref.at[:, idx], out_tile.astype(jnp.float32), mask=mask)\n",
    "\n",
    "    _ = jax.lax.fori_loop(0, n_col_blocks, out_body, None)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def softmax(logits):\n",
    "    n_row_blocks = pl.cdiv(logits.shape[0], BLOCK_M)\n",
    "    n_col_blocks = pl.cdiv(logits.shape[1], BLOCK_N)\n",
    "    return pl.pallas_call(\n",
    "        partial(softmax_kernel, n_col_blocks=n_col_blocks, n_rows=logits.shape[0], n_cols=logits.shape[1]),\n",
    "        out_shape=jax.ShapeDtypeStruct(logits.shape, jnp.float32),\n",
    "        grid=(n_row_blocks,),\n",
    "        in_specs=[pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0))],\n",
    "        out_specs=pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0)),\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES,\n",
    "        ),\n",
    "    )(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cad5e1",
   "metadata": {},
   "source": [
    "\n",
    "## Performance\n",
    "\n",
    "Let's compare our performance with the out-of-the-box softmax implementation provided by Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af4057a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Softmax: 0.44 ms\n",
      "Pallas Softmax: 24.15 ms\n",
      "Speedup (jax / pallas): 0.02x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, iters=10):\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn(*args)\n",
    "        out.block_until_ready()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    times.sort()\n",
    "    return times[len(times) // 2]\n",
    "\n",
    "\n",
    "d = 1024\n",
    "key = jax.random.key(0)\n",
    "logits = jax.random.normal(shape=(d, d), key=key)\n",
    "\n",
    "out_jax = jax.nn.softmax(logits)\n",
    "out_online = online_softmax(logits)\n",
    "out_pallas = softmax(logits)\n",
    "\n",
    "assert jnp.allclose(jnp.squeeze(out_jax), out_pallas)\n",
    "assert jnp.allclose(jnp.squeeze(out_jax), out_online)\n",
    "\n",
    "softmax_jit = jax.jit(jax.nn.softmax)\n",
    "\n",
    "_ = softmax_jit(logits).block_until_ready()\n",
    "_ = softmax(logits).block_until_ready()\n",
    "\n",
    "t_jax = bench(softmax_jit, logits)\n",
    "t_pallas = bench(softmax, logits)\n",
    "\n",
    "print(f\"Jax Softmax: {t_jax*1e3:.2f} ms\")\n",
    "print(f\"Pallas Softmax: {t_pallas*1e3:.2f} ms\")\n",
    "print(f\"Speedup (jax / pallas): {t_jax / t_pallas:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33ef43",
   "metadata": {},
   "source": [
    "Our softmax kernel is significantly slower than the Jax out-of-the-box implementation. This is not surprising considering that Jax uses Nvidia's CUDA kernels under the hood, which are highly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3c7f2",
   "metadata": {},
   "source": [
    "\n",
    "## Backward Pass\n",
    "\n",
    "Let's first derive the expression for the backward pass.\n",
    "\n",
    "Let $x\\in\\mathbb{R}^n, y=\\mathrm{softmax}(x)$ with\n",
    "$y_i=\\frac{e^{x_i}}{\\sum_{k=1}^n e^{x_k}}.$\n",
    "Assume an upstream gradient $g=\\frac{\\partial L}{\\partial y}$ is given, and we want $\\frac{\\partial L}{\\partial x}$.\n",
    "\n",
    "First compute the Jacobian of softmax. Write $S=\\sum_k e^{x_k}$, so $y_i=e^{x_i}/S$.\n",
    "Differentiate $y_i$ w.r.t. $x_j$:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\frac{\\partial}{\\partial x_j}\\left(\\frac{e^{x_i}}{S}\\right)\n",
    "=\\frac{\\delta_{ij}e^{x_i}\\,S - e^{x_i}\\,\\frac{\\partial S}{\\partial x_j}}{S^2}.\n",
    "$$\n",
    "But $\\frac{\\partial S}{\\partial x_j}=e^{x_j}$. \n",
    "Substitute:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\frac{\\delta_{ij}e^{x_i}S - e^{x_i}e^{x_j}}{S^2}\n",
    "=\\delta_{ij}\\frac{e^{x_i}}{S} - \\frac{e^{x_i}}{S}\\frac{e^{x_j}}{S}\n",
    "=\\delta_{ij}y_i - y_i y_j.\n",
    "$$\n",
    "So the Jacobian is\n",
    "$$\n",
    "J_{ij} = \\frac{\\partial y_i}{\\partial x_j}= y_i(\\delta_{ij}-y_j).\n",
    "$$\n",
    "\n",
    "Now apply the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}\n",
    "=\\sum_{i=1}^n \\frac{\\partial L}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_j}\n",
    "=\\sum_i g_i\\left(\\delta_{ij}y_i - y_i y_j\\right).\n",
    "$$\n",
    "Split the sum:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j}\n",
    "= g_j y_j - y_j\\sum_i g_i y_i\n",
    "= y_j\\left(g_j - \\sum_i g_i y_i\\right).\n",
    "$$\n",
    "\n",
    "Finally, in vector form:\n",
    "$$\n",
    "\\;\\frac{\\partial L}{\\partial x} = y \\odot (g - \\langle g, y\\rangle)\\;\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e804ae9",
   "metadata": {},
   "source": [
    "The kernel for the backward pass can be implemented in two steps. First, we can compute the inner product $ \\langle g, y\\rangle $. Next, an elementwise operation to compute the final expression. Since this is a binary classifier, both the upstream gradient $g$ and the output $y$ will be of shape (B, C) where B is the batch size and C is the number of classes. Since C = 2, we only need to tile our kernel along the B axis, simplifying our implementation greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c430de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward_kernel(y_ref, dy_ref, dx_ref):\n",
    "    # compute the inner product <g_ref, y_ref>\n",
    "    dy_reg = plgpu.load(dy_ref)\n",
    "    y_reg = plgpu.load(y_ref)\n",
    "    g_dot_y = jnp.sum(dy_reg * y_reg, axis=1)\n",
    "\n",
    "    # Compute the output block\n",
    "    output_reg = y_reg * ( dy_reg - g_dot_y[:, None] )\n",
    "    plgpu.store(dx_ref, output_reg)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def softmax_backward(y, dy):\n",
    "    M, N = y.shape\n",
    "\n",
    "    grid = (pl.cdiv(M, BLOCK_M),)\n",
    "    out_shape = jax.ShapeDtypeStruct((M, N), y.dtype)\n",
    "\n",
    "    return pl.pallas_call(\n",
    "        softmax_backward_kernel,\n",
    "        out_shape=out_shape,\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # y\n",
    "            pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # dy \n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((BLOCK_M, N), lambda i: (i, 0)),  # dx\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES,\n",
    "        ),\n",
    "    )(y, dy)\n",
    "\n",
    "\n",
    "@jax.custom_vjp\n",
    "def softmax_pallas(x):\n",
    "    return softmax(x)\n",
    "\n",
    "\n",
    "def softmax_fwd(x):\n",
    "    y = softmax(x)\n",
    "    return y, y\n",
    "\n",
    "\n",
    "def softmax_bwd(saved_y, dy):\n",
    "    (y,) = (saved_y,)\n",
    "    dx = softmax_backward(y, dy)\n",
    "    return (dx,)\n",
    "\n",
    "\n",
    "softmax_pallas.defvjp(softmax_fwd, softmax_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c84ce",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Evaluate our Kernel\n",
    "\n",
    "We will attempt to train a binary classifier model on some synthetic data. Let's start by generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fca27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, E = 256, 24 # (batch size, number of features)\n",
    "num_classes = 2\n",
    "x = jax.random.normal(jax.random.key(1000), (B, E))\n",
    "class_ids = (x[:, 0] > 0).astype(jnp.int32)\n",
    "y = class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec04b83",
   "metadata": {},
   "source": [
    "Next, let's define our binary classifier and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac92b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import flax.nnx as nnx\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    in_dim: int\n",
    "    hidden_dim: int\n",
    "    out_dim: int\n",
    "\n",
    "class Model(nnx.Module):\n",
    "    def __init__(self, config: ModelConfig, rngs: nnx.Rngs):\n",
    "        self.config = config\n",
    "        self.l1 = nnx.Linear(config.in_dim, config.hidden_dim, rngs=rngs)\n",
    "        self.l2 = nnx.Linear(config.hidden_dim, config.out_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(-1, x.shape[-1])\n",
    "        x = self.l1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    probs = softmax_pallas(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss, (probs, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d8e26",
   "metadata": {},
   "source": [
    "Before we train the model, let's first test if our backward pass kernel is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0e1116b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def loss_from_logits_pallas(logits, y):\n",
    "    probs = softmax_pallas(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss\n",
    "\n",
    "def loss_from_logits_gt(logits, y):\n",
    "    probs = jax.nn.softmax(logits)\n",
    "    labels = y.reshape(-1)\n",
    "    one_hot = jax.nn.one_hot(labels, probs.shape[-1], dtype=probs.dtype)\n",
    "    loss = -jnp.mean(jnp.sum(one_hot * jnp.log(probs + 1e-9), axis=-1))\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def verify(model, x, y):\n",
    "    logits = model(x)\n",
    "    d_logits_pallas = jax.grad(loss_from_logits_pallas)(logits, y)\n",
    "    d_logits_gt = jax.grad(loss_from_logits_gt)(logits, y)\n",
    "    return jnp.allclose(d_logits_pallas, d_logits_gt)\n",
    "\n",
    "\n",
    "default = jax.random.key(69)\n",
    "rngs = nnx.Rngs(default=default) \n",
    "\n",
    "config = ModelConfig(in_dim=E, hidden_dim=E * 4, out_dim=num_classes)\n",
    "model = Model(config, rngs)\n",
    "model.train(add_noise=False)\n",
    "\n",
    "print(verify(model, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1764a",
   "metadata": {},
   "source": [
    "Excellent! Looks like our backward pass kernel works correctly. Finally, let's overfit the model on our toy dataset using our softmax kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58d2d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss=0.716331958770752\n",
      "iter 1: loss=0.578141987323761\n",
      "iter 2: loss=1.0508484840393066\n",
      "iter 3: loss=0.13638077676296234\n",
      "iter 4: loss=0.35514646768569946\n",
      "iter 5: loss=0.1290615051984787\n",
      "iter 6: loss=0.06484488397836685\n",
      "iter 7: loss=0.05634478107094765\n",
      "iter 8: loss=0.05398242175579071\n",
      "iter 9: loss=0.032313257455825806\n",
      "iter 10: loss=0.016733389347791672\n",
      "iter 11: loss=0.010100538842380047\n",
      "iter 12: loss=0.003886598628014326\n",
      "iter 13: loss=0.0019385517807677388\n",
      "iter 14: loss=0.0014023631811141968\n"
     ]
    }
   ],
   "source": [
    "import optax \n",
    "\n",
    "@nnx.jit\n",
    "def step(state, x, y):\n",
    "    (loss, (y_pred, logits)), grads = nnx.value_and_grad(\n",
    "        loss_fn, has_aux=True)(state.model, x, y)\n",
    "    state.update(grads)\n",
    "    return loss\n",
    "\n",
    "tx = optax.adam(1e-1)\n",
    "state = nnx.Optimizer(model, tx)\n",
    "\n",
    "iters = 15\n",
    "for i in range(iters):\n",
    "    loss = step(state, x, y)\n",
    "    print(f\"iter {i}: loss={loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ab243",
   "metadata": {},
   "source": [
    "We were able to successfully overfit our toy dataset. In the next post we'll build on this and implement a custom Pallas kernel for computing self-attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
