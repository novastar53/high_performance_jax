{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a86036",
   "metadata": {},
   "source": [
    "# How to Write a Softmax GPU Kernel in Pallas\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1062b",
   "metadata": {},
   "source": [
    "## Softmax Operation\n",
    "\n",
    "Given an input vector $z = (z_1, ..., z_n) \\in R^n$, the softmax function σ : R^n → (0,1)^n produces a probability distribution over the n entries:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^n \\exp(z_j)} \\quad\\text{for } i=1,\\dots,n.\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Each output is positive and they sum to 1: ∑_i σ(z)_i = 1.\n",
    "- Softmax is invariant to shifts: σ(z) = σ(z + c·1) for any scalar c. For numerical stability one commonly uses\n",
    "$$\n",
    "\\sigma(z)_i = \\frac{\\exp(z_i - \\max_j z_j)}{\\sum_{k=1}^n \\exp(z_k - \\max_j z_j)}.\n",
    "$$\n",
    "\n",
    "\n",
    "Use cases: converts logits to probabilities (classification), used with cross-entropy loss for efficient training (softmax + log-loss simplifications)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00557c1",
   "metadata": {},
   "source": [
    "\n",
    "## Online Softmax\n",
    "\n",
    "The online softmax algorithm is a numerically stable method for computing softmax in a single pass, without storing all exponentials in memory. This is particularly useful for processing large sequences or implementing efficient GPU kernels.\n",
    "\n",
    "### Algorithm Description\n",
    "\n",
    "The key idea is to maintain running statistics (maximum value and sum of exponentials) as we iterate through the input:\n",
    "\n",
    "1. **Track the running maximum**: As we process elements, we keep track of the largest value seen so far.\n",
    "2. **Update the sum of exponentials**: When we encounter a new maximum, we rescale previous exponentials and add the new ones.\n",
    "3. **Compute probabilities**: Finally, divide each exponential by the total sum.\n",
    "\n",
    "This avoids numerical overflow/underflow and allows us to process data in blocks without materializing the full exponential array.\n",
    "\n",
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94b8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.experimental.pallas as pl\n",
    "from jax.experimental.pallas import triton as plgpu\n",
    "\n",
    "\n",
    "def online_softmax(logits):\n",
    "    max_rows = jnp.max(logits, axis=-1)\n",
    "    s = jnp.exp(logits - max_rows[..., None])\n",
    "    l = jnp.sum(s, axis=-1)\n",
    "    l = l[..., None]\n",
    "    return s / l "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea77d7a",
   "metadata": {},
   "source": [
    "The next step is to convert this into an efficient GPU kernel. See my previous post on writing an efficient matrix multiplication kernel here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44070d44",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPRET_MODE = True # Set to False on GPU\n",
    "\n",
    "# Pallas softmax\n",
    "BLOCK_M = 64\n",
    "BLOCK_N = 64\n",
    "NUM_WARPS = 4\n",
    "NUM_STAGES = 3\n",
    "\n",
    "\n",
    "def softmax_kernel(x_ref, out_ref, *, n_col_blocks, n_rows, n_cols):\n",
    "    max_reg = jnp.full((BLOCK_M,), -jnp.inf, dtype=jnp.float32) \n",
    "    l_reg = jnp.zeros((BLOCK_M,), dtype=jnp.float32) \n",
    "    row_ids = pl.program_id(0) * BLOCK_M + jnp.arange(BLOCK_M)\n",
    "    row_mask = row_ids < n_rows\n",
    "\n",
    "    def stats_body(t, args):\n",
    "        max_reg, l_reg = args\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        max_tile = jnp.max(x_tile, axis=-1)\n",
    "        max_new = jnp.maximum(max_reg, max_tile)\n",
    "        l_update = l_reg * jnp.exp(max_reg - max_new) + jnp.sum(\n",
    "            jnp.exp(x_tile - max_new[:, None]), axis=-1\n",
    "        )\n",
    "        return max_new, l_update\n",
    "        \n",
    "    max_reg, l_reg = jax.lax.fori_loop(0, n_col_blocks, stats_body, (max_reg, l_reg))\n",
    "\n",
    "    def out_body(t, _):\n",
    "        idx = pl.dslice(t * BLOCK_N, BLOCK_N)\n",
    "        col_ids = t * BLOCK_N + jnp.arange(BLOCK_N)\n",
    "        cols_mask = col_ids < n_cols\n",
    "        mask = row_mask[:, None] & cols_mask[None, :]\n",
    "\n",
    "        x_tile = plgpu.load(\n",
    "            x_ref.at[:, idx],\n",
    "            mask=mask,\n",
    "            other=-jnp.inf,\n",
    "        ).astype(jnp.float32)\n",
    "        out_tile = jnp.exp(x_tile - max_reg[:, None]) / l_reg[:, None]\n",
    "        plgpu.store(out_ref.at[:, idx], out_tile.astype(jnp.float32), mask=mask)\n",
    "\n",
    "    _ = jax.lax.fori_loop(0, n_col_blocks, out_body, None)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def softmax(logits):\n",
    "    n_row_blocks = pl.cdiv(logits.shape[0], BLOCK_M)\n",
    "    n_col_blocks = pl.cdiv(logits.shape[1], BLOCK_N)\n",
    "    return pl.pallas_call(\n",
    "        partial(softmax_kernel, n_col_blocks=n_col_blocks, n_rows=logits.shape[0], n_cols=logits.shape[1]),\n",
    "        out_shape=jax.ShapeDtypeStruct(logits.shape, jnp.float32),\n",
    "        grid=(n_row_blocks,),\n",
    "        in_specs=[pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0))],\n",
    "        out_specs=pl.BlockSpec((BLOCK_M, logits.shape[1]), lambda i: (i, 0)),\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES,\n",
    "        ),\n",
    "    )(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cad5e1",
   "metadata": {},
   "source": [
    "\n",
    "## Performance\n",
    "\n",
    "Let's compare our performance with the out-of-the-box softmax implementation provided by Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4057a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Softmax: 0.83 ms\n",
      "Pallas Softmax: 28.84 ms\n",
      "Speedup (jax / pallas): 0.03x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def bench(fn, *args, iters=10):\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn(*args)\n",
    "        out.block_until_ready()\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "    times.sort()\n",
    "    return times[len(times) // 2]\n",
    "\n",
    "\n",
    "d = 1024\n",
    "key = jax.random.key(0)\n",
    "logits = jax.random.normal(shape=(d, d), key=key)\n",
    "\n",
    "out_jax = jax.nn.softmax(logits)\n",
    "out_pallas = softmax(logits)\n",
    "\n",
    "assert jnp.allclose(jnp.squeeze(out_jax), out_pallas)\n",
    "\n",
    "softmax_jit = jax.jit(jax.nn.softmax)\n",
    "\n",
    "_ = softmax_jit(logits).block_until_ready()\n",
    "_ = softmax(logits).block_until_ready()\n",
    "\n",
    "t_jax = bench(softmax_jit, logits)\n",
    "t_pallas = bench(softmax, logits)\n",
    "\n",
    "print(f\"Jax Softmax: {t_jax*1e3:.2f} ms\")\n",
    "print(f\"Pallas Softmax: {t_pallas*1e3:.2f} ms\")\n",
    "print(f\"Speedup (jax / pallas): {t_jax / t_pallas:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d3c7f2",
   "metadata": {},
   "source": [
    "\n",
    "## Backward Pass\n",
    "\n",
    "\n",
    "### Gradient / Jacobian:\n",
    "Let s = σ(z). The Jacobian matrix J with entries $∂σ_i/∂z_j$ is\n",
    "$$\n",
    "\\frac{\\partial \\sigma_i}{\\partial z_j} = s_i(\\delta_{ij} - s_j),\n",
    "$$\n",
    "where $δ_{ij}$ is the Kronecker delta. Equivalently, $J = diag(s) - s s^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c84ce",
   "metadata": {},
   "source": [
    "\n",
    "## Let's Evaluate our Kernel in a Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d2d606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnnx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnnx\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m D, B, T, C =  \u001b[32m1000\u001b[39m, \u001b[43mconfig\u001b[49m.n_experts, \u001b[32m5\u001b[39m, config.n_embed \n\u001b[32m      7\u001b[39m default = jax.random.key(\u001b[32m69\u001b[39m)\n\u001b[32m      8\u001b[39m gate_noise = jax.random.key(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "\n",
    "D, B, T, C =  1000, config.n_experts, 5, config.n_embed \n",
    "\n",
    "default = jax.random.key(69)\n",
    "gate_noise = jax.random.key(42)\n",
    "rngs = nnx.Rngs(default=default, gate_noise=gate_noise)\n",
    "\n",
    "#model = MOE(config, rngs)\n",
    "#model.train(add_noise=False)\n",
    "#tx = optax.adam(1e-2)\n",
    "#state = nnx.Optimizer(model, tx)\n",
    "\n",
    "x = jax.random.normal(jax.random.key(1000), (D, B, T, C))\n",
    "\n",
    "expert_ids = (x[:, :, :, 0] > 0).astype(jnp.int32)[..., None]\n",
    "t = [\n",
    "    jax.random.normal(jax.random.key(2000), (C, C)),\n",
    "    jax.random.normal(jax.random.key(3000), (C, C)),\n",
    "]\n",
    "\n",
    "def transform(xi, eid):\n",
    "    return jnp.where(eid == 1, xi @ t[0], xi @ t[1])\n",
    "\n",
    "y = jax.vmap(lambda xi, ei: transform(xi, ei))(x, expert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3895ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
