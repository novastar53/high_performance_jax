{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "# Batch Normalization: Spline Partition Perspective\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates key insights from the paper \"Batch Normalization Explained\" which shows that:\n",
    "\n",
    "1. **Spline Partitions**: ReLU networks partition input space into linear regions where the network behaves as an affine transformation\n",
    "2. **BN Alignment**: Batch Normalization moves partition boundaries to align with training data\n",
    "3. **Margin Effect**: BN increases decision boundary margins, improving generalization\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What are Spline Partitions?\n",
    "\n",
    "Modern networks use piecewise linear activations (ReLU, Leaky-ReLU). The composition of piecewise linear functions creates a **Continuous Piecewise Affine (CPA) spline** that partitions input space into:\n",
    "\n",
    "- **Linear regions**: Convex regions where network acts as simple linear transformation\n",
    "- **Partition boundaries**: Hyperplanes defined by where neuron pre-activations equal zero\n",
    "- **Folding process**: Multiple layers fold and refold these boundaries creating complex partitions\n",
    "\n",
    "### How Does BN Help?\n",
    "\n",
    "BN doesn't just normalize activations - it **smartly repositions partition boundaries**:\n",
    "\n",
    "- **Without BN**: Boundaries spread uniformly across input space (wasted capacity)\n",
    "- **With BN**: Boundaries concentrate densely around training data (efficient use of capacity)\n",
    "- **Result**: Better generalization through larger effective margins\n",
    "\n",
    "## Reference\n",
    "\n",
    "- Paper: [[Batch Normalization Explained.pdf]] in ML Notes vault\n",
    "- Deepkit utilities: `spline_partitions.py`, `boundary_analysis.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup & Imports\n",
    "\n",
    "Import necessary libraries and configure plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles, make_moons, make_classification\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from deepkit.spline_partitions import (\n",
    "    visualize_partitions_2d,\n",
    "    compute_partition_entropy\n",
    ")\n",
    "from deepkit.boundary_analysis import (\n",
    "    compute_decision_margin,\n",
    "    compute_boundary_alignment,\n",
    "    compute_gradient_norm_statistics\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "## Dataset: 2D Synthetic Classification\n",
    "\n",
    "We use 2D datasets to visualize spline partitions and decision boundaries clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D dataset with clear non-linear decision boundary\n",
    "X, y = make_circles(\n",
    "    n_samples=500,\n",
    "    noise=0.1,\n",
    "    factor=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X = jnp.array(X)\n",
    "y = jnp.array(y)\n",
    "\n",
    "# Visualize dataset\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', alpha=0.6, edgecolors='black')\n",
    "ax.set_title('2D Synthetic Dataset: Concentric Circles')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Dataset shape: {X.shape}')\n",
    "print(f'Classes: {jnp.unique(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "Define two simple MLP architectures:\n",
    "1. **Without BN**: Standard ReLU network\n",
    "2. **With BN**: Same architecture with BatchNorm after each linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithoutBN(nnx.Module):\n",
    "    \"\"\"Simple MLP without Batch Normalization\"\"\"\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.hidden1 = nnx.Linear(2, 64, rngs=rngs)\n",
    "        self.hidden2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.output = nnx.Linear(64, 2, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = nnx.relu(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "class MLPWithBN(nnx.Module):\n",
    "    \"\"\"Same MLP with Batch Normalization\"\"\"\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.hidden1 = nnx.Linear(2, 64, rngs=rngs)\n",
    "        self.bn1 = nnx.BatchNorm(64, rngs=rngs)\n",
    "        self.hidden2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.bn2 = nnx.BatchNorm(64, rngs=rngs)\n",
    "        self.output = nnx.Linear(64, 2, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nnx.relu(x)\n",
    "        return self.output(x)\n",
    "\n",
    "# Initialize models\n",
    "key = jax.random.key(1337)\n",
    "model_no_bn = MLPWithoutBN(rngs=nnx.Rngs(key))\n",
    "model_with_bn = MLPWithBN(rngs=nnx.Rngs(key))\n",
    "\n",
    "print('Models initialized successfully!')\n",
    "print(f'Without BN parameters: {sum(x.size for x in jax.tree_util.tree_leaves(model_no_bn) if isinstance(x, jnp.ndarray)):,}')\n",
    "print(f'With BN parameters: {sum(x.size for x in jax.tree_util.tree_leaves(model_with_bn) if isinstance(x, jnp.ndarray)):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "untrained-visualization",
   "metadata": {},
   "source": [
    "## Visualization: Untrained Networks\n",
    "\n",
    "Before training, let's visualize the random decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-untrained",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both models to eval mode for stable visualization\n",
    "model_no_bn.eval()\n",
    "model_with_bn.eval()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_no_bn, X, y,\n",
    "    ax=axes[0],\n",
    "    title='Without Batch Normalization (Untrained)'\n",
    ")\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_with_bn, X, y,\n",
    "    ax=axes[1],\n",
    "    title='With Batch Normalization (Untrained)'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Notice: Both untrained models show random decision boundaries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Configure training with identical hyperparameters for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Optimizers\n",
    "opt_no_bn = nnx.Optimizer(model_no_bn, optax.sgd(learning_rate=lr, momentum=momentum), wrt=nnx.Param)\n",
    "opt_with_bn = nnx.Optimizer(model_with_bn, optax.sgd(learning_rate=lr, momentum=momentum), wrt=nnx.Param)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(model, batch, labels):\n",
    "    logits = model(batch)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "    return loss\n",
    "\n",
    "# Accuracy function\n",
    "@nnx.jit\n",
    "def accuracy(model, batch, labels):\n",
    "    logits = model(batch)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    return jnp.mean(preds == labels)\n",
    "\n",
    "# Training step\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch, labels):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, labels)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "print('Training configured!')\n",
    "print(f'Learning rate: {lr}')\n",
    "print(f'Epochs: {num_epochs}')\n",
    "print(f'Batch size: {batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-loop",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train both models simultaneously and track their progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'no_bn': {'loss': [], 'acc': []},\n",
    "    'with_bn': {'loss': [], 'acc': []}\n",
    "}\n",
    "\n",
    "# Convert to batches\n",
    "num_batches = len(X) // batch_size\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model_no_bn.train()\n",
    "        model_with_bn.train()\n",
    "        \n",
    "        epoch_loss_no_bn = 0.0\n",
    "        epoch_loss_with_bn = 0.0\n",
    "        epoch_acc_no_bn = 0.0\n",
    "        epoch_acc_with_bn = 0.0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_X = X[start_idx:end_idx]\n",
    "            batch_y = y[start_idx:end_idx]\n",
    "            \n",
    "            # Train both models\n",
    "            loss_no_bn = train_step(model_no_bn, opt_no_bn, batch_X, batch_y)\n",
    "            loss_with_bn = train_step(model_with_bn, opt_with_bn, batch_X, batch_y)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            acc_no_bn = accuracy(model_no_bn, batch_X, batch_y)\n",
    "            acc_with_bn = accuracy(model_with_bn, batch_X, batch_y)\n",
    "            \n",
    "            epoch_loss_no_bn += float(loss_no_bn)\n",
    "            epoch_loss_with_bn += float(loss_with_bn)\n",
    "            epoch_acc_no_bn += float(acc_no_bn)\n",
    "            epoch_acc_with_bn += float(acc_with_bn)\n",
    "        \n",
    "        # Average metrics\n",
    "        history['no_bn']['loss'].append(epoch_loss_no_bn / num_batches)\n",
    "        history['with_bn']['loss'].append(epoch_loss_with_bn / num_batches)\n",
    "        history['no_bn']['acc'].append(epoch_acc_no_bn / num_batches)\n",
    "        history['with_bn']['acc'].append(epoch_acc_with_bn / num_batches)\n",
    "        \n",
    "        # Visualize progress\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "            \n",
    "            # Loss curves\n",
    "            axes[0].plot(history['no_bn']['loss'], label='No BN', alpha=0.7)\n",
    "            axes[0].plot(history['with_bn']['loss'], label='With BN', alpha=0.7)\n",
    "            axes[0].set_title('Training Loss')\n",
    "            axes[0].set_xlabel('Epoch')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Accuracy curves\n",
    "            axes[1].plot(history['no_bn']['acc'], label='No BN', alpha=0.7)\n",
    "            axes[1].plot(history['with_bn']['acc'], label='With BN', alpha=0.7)\n",
    "            axes[1].set_title('Training Accuracy')\n",
    "            axes[1].set_xlabel('Epoch')\n",
    "            axes[1].set_ylabel('Accuracy')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            print(f\"  No BN   - Loss: {history['no_bn']['loss'][-1]:.4f}, Acc: {history['no_bn']['acc'][-1]:.4f}\")\n",
    "            print(f\"  With BN - Loss: {history['with_bn']['loss'][-1]:.4f}, Acc: {history['with_bn']['acc'][-1]:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('\\nTraining interrupted!')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-visualization",
   "metadata": {},
   "source": [
    "## Visualization: Trained Networks\n",
    "\n",
    "Now let's see how the decision boundaries have changed after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-trained",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set models to eval mode for visualization\n",
    "model_no_bn.eval()\n",
    "model_with_bn.eval()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Untrained vs trained for no BN\n",
    "key_untrained = jax.random.key(1337)\n",
    "model_untrained = MLPWithoutBN(rngs=nnx.Rngs(key_untrained))\n",
    "model_untrained.eval()\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_untrained, X, y,\n",
    "    ax=axes[0, 0],\n",
    "    title='No BN: Before Training'\n",
    ")\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_no_bn, X, y,\n",
    "    ax=axes[0, 1],\n",
    "    title='No BN: After Training'\n",
    ")\n",
    "\n",
    "# Untrained vs trained for with BN\n",
    "model_bn_untrained = MLPWithBN(rngs=nnx.Rngs(key_untrained))\n",
    "model_bn_untrained.eval()\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_bn_untrained, X, y,\n",
    "    ax=axes[1, 0],\n",
    "    title='With BN: Before Training'\n",
    ")\n",
    "\n",
    "visualize_partitions_2d(\n",
    "    model_with_bn, X, y,\n",
    "    ax=axes[1, 1],\n",
    "    title='With BN: After Training'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Observe how BN affects the decision boundary geometry!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boundary-alignment",
   "metadata": {},
   "source": [
    "## Analysis: Boundary Alignment\n",
    "\n",
    "Quantify how decision boundaries align with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute boundary alignment metrics\n",
    "mean_dist_no_bn, density_no_bn = compute_boundary_alignment(model_no_bn, X, y)\n",
    "mean_dist_with_bn, density_with_bn = compute_boundary_alignment(model_with_bn, X, y)\n",
    "\n",
    "print('=== Boundary Alignment Analysis ===')\n",
    "print(f'\\nWithout Batch Normalization:')\n",
    "print(f'  Mean distance to boundary: {mean_dist_no_bn:.4f}')\n",
    "print(f'  Boundary density: {density_no_bn:.4f}')\n",
    "\n",
    "print(f'\\nWith Batch Normalization:')\n",
    "print(f'  Mean distance to boundary: {mean_dist_with_bn:.4f}')\n",
    "print(f'  Boundary density: {density_with_bn:.4f}')\n",
    "\n",
    "print(f'\\nKey Insights:')\n",
    "if mean_dist_with_bn < mean_dist_no_bn:\n",
    "    improvement = (mean_dist_no_bn - mean_dist_with_bn) / mean_dist_no_bn * 100\n",
    "    print(f'  ✓ BN reduces mean distance by {improvement:.1f}%')\n",
    "    print(f'  → Boundaries are closer to data (better alignment)')\n",
    "else:\n",
    "    print(f'  ✗ BN increased distance (unexpected)')\n",
    "\n",
    "if density_with_bn > density_no_bn:\n",
    "    print(f'  ✓ BN increases boundary density')\n",
    "    print(f'  → More decision boundaries near data points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "margin-analysis",
   "metadata": {},
   "source": [
    "## Analysis: Decision Margins\n",
    "\n",
    "Measure the margin - distance from decision boundary to nearest training point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute decision margins\n",
    "margin_no_bn = compute_decision_margin(model_no_bn, X, y, num_steps=30)\n",
    "margin_with_bn = compute_decision_margin(model_with_bn, X, y, num_steps=30)\n",
    "\n",
    "print('=== Decision Margin Analysis ===')\n",
    "print(f'\\nWithout Batch Normalization:')\n",
    "print(f'  Decision margin: {margin_no_bn:.4f}')\n",
    "\n",
    "print(f'\\nWith Batch Normalization:')\n",
    "print(f'  Decision margin: {margin_with_bn:.4f}')\n",
    "\n",
    "print(f'\\nComparison:')\n",
    "if margin_with_bn > margin_no_bn:\n",
    "    improvement = (margin_with_bn / margin_no_bn - 1) * 100\n",
    "    print(f'  ✓ BN increases margin by {improvement:.1f}%')\n",
    "    print(f'  → Larger margins typically indicate better generalization')\n",
    "elif margin_with_bn < margin_no_bn:\n",
    "    decrease = (margin_no_bn / margin_with_bn - 1) * 100\n",
    "    print(f'  ✗ BN decreases margin by {decrease:.1f}%')\n",
    "    print(f'  → This may indicate overfitting or other issues')\n",
    "else:\n",
    "    print(f'  = Margins are similar')\n",
    "\n",
    "# Visualize margins\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "models = ['No BN', 'With BN']\n",
    "margins = [margin_no_bn, margin_with_bn]\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "bars = ax.bar(models, margins, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Decision Margin')\n",
    "ax.set_title('Decision Margin Comparison')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, margin in zip(bars, margins):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{margin:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradient-analysis",
   "metadata": {},
   "source": [
    "## Analysis: Gradient Norms\n",
    "\n",
    "Examine gradient statistics as a proxy for loss landscape smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-gradients",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient statistics\n",
    "grad_stats_no_bn = compute_gradient_norm_statistics(model_no_bn, X, y)\n",
    "grad_stats_with_bn = compute_gradient_norm_statistics(model_with_bn, X, y)\n",
    "\n",
    "print('=== Gradient Norm Analysis ===')\n",
    "print(f'\\nWithout Batch Normalization:')\n",
    "print(f'  Mean gradient norm: {grad_stats_no_bn[\"mean_grad_norm\"]:.4f}')\n",
    "\n",
    "print(f'\\nWith Batch Normalization:')\n",
    "print(f'  Mean gradient norm: {grad_stats_with_bn[\"mean_grad_norm\"]:.4f}')\n",
    "\n",
    "print(f'\\nInterpretation:')\n",
    "if grad_stats_with_bn['mean_grad_norm'] < grad_stats_no_bn['mean_grad_norm']:\n",
    "    reduction = (1 - grad_stats_with_bn['mean_grad_norm'] / grad_stats_no_bn['mean_grad_norm']) * 100\n",
    "    print(f'  ✓ BN reduces gradient norms by {reduction:.1f}%')\n",
    "    print(f'  → Smoother loss landscape (easier optimization)')\n",
    "else:\n",
    "    increase = (grad_stats_with_bn['mean_grad_norm'] / grad_stats_no_bn['mean_grad_norm'] - 1) * 100\n",
    "    print(f'  BN increases gradient norms by {increase:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary & Key Findings\n",
    "\n",
    "### What We Observed\n",
    "\n",
    "1. **Spline Partitions**: ReLU networks create complex piecewise-linear decision boundaries\n",
    "\n",
    "2. **BN Alignment**: Batch Normalization affects where partition boundaries form\n",
    "   - Boundaries should be more densely packed around training data\n",
    "   - This represents more efficient use of network capacity\n",
    "\n",
    "3. **Decision Margins**: BN affects the distance from decision boundaries to data\n",
    "   - Larger margins generally correlate with better generalization\n",
    "   - The \"jitter\" effect mentioned in the paper\n",
    "\n",
    "4. **Optimization**: Gradient norms differ with/without BN\n",
    "   - Reflects changes in loss landscape geometry\n",
    "   - Smoother landscape = easier optimization\n",
    "\n",
    "### Connection to Paper\n",
    "\n",
    "The paper's key insight: **Batch Normalization is not just about normalization** - it's about geometrically repositioning the spline partition boundaries to be more useful for the actual data distribution.\n",
    "\n",
    "- **Without BN**: Boundaries spread uniformly (wasted capacity in empty regions)\n",
    "- **With BN**: Boundaries concentrate around data (efficient capacity utilization)\n",
    "\n",
    "This geometric effect, combined with the \"jitter\" from batch statistics, creates larger effective margins and better generalization.\n",
    "\n",
    "### Further Experiments\n",
    "\n",
    "To extend this work:\n",
    "- Try different 2D datasets (moons, spirals)\n",
    "- Vary network depth and width\n",
    "- Compare with other normalization techniques (Layer Norm, Group Norm)\n",
    "- Analyze how effects scale with dimensionality\n",
    "\n",
    "### References\n",
    "\n",
    "- Paper: Batch Normalization Explained\n",
    "- Code: `deepkit.spline_partitions`, `deepkit.boundary_analysis`\n",
    "- Related: Santurkar et al \"How Does Batch Normalization Help Optimization?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
