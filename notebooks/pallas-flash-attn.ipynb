{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Write a Flash Attention Kernel in Pallas\n",
    "## Introduction\n",
    "\n",
    "In the previous posts in this series, we learnt how to write a [matrix multiplication kernel](https://blog.vikrampawar.com/pallas-matmul.html) and a [softmax kernel](https://blog.vikrampawar.com/pallas-softmax.html) using Pallas. Building on them, we will write a fused self-attention kernel, since self-attention includes both operations. Self-attention is a major bottleneck in deep learning architectures due to its $O(N^2)$ memory requirement, which is the bottleneck rather than computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Attention\n",
    "\n",
    "Mathematically, the self-attention operation is `softmax(QK^T / √d) @ V`, where  Q is a set of queries, K is a set of keys and V is a set of values. \n",
    "\n",
    "The Queries (Q) are usually a tensor of shape `(B, H, T, D)`, where `B` is batch size, `H` is number of heads, `T` is sequence length, and `D` is the embedding dimension (or head dimension). Each query vector represents a position in the sequence that attends to keys. Keys are used to compute attention scores with queries while values are the information retrieved based on attention weights.\n",
    "The normalizing constant $\\sqrt{d}$ (where $d = D$ is applied to scale down the dot-product attention scores \\(QK^T\\). Without it, the variance of the attention logits increases with \\(D\\), leading to a softmax distribution that becomes overly peaked (approaching one-hot) for large \\(D\\). This makes gradients unstable and training harder. Scaling by $1/\\sqrt{d}$ stabilizes the variance, ensuring the softmax behaves similarly across different embedding dimensions, improving optimization and generalization.\n",
    "\n",
    "To understand the basics of self-attention in detail, [here's](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) an excellent blogpost by Sebastian Raschka [6]. \n",
    "\n",
    "In a naive implementation, it would require $O(N^2)$ memory  for sequence length $N$ to instantiate the full attention matrix. This is highly inefficient on GPUs due to the memory bottleneck - GPUs are generally much more efficient at performing the matrix multiplications than transferring data from HBM to shared memory (SMEM). To understand this tradeoff, check out my matrix multiplication kernel blogpost which has a primer on the GPU memory hierarchy [here](https://blog.vikrampawar.com/pallas-matmul.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def mha_reference(q, k, v):\n",
    "    \"\"\"Reference multi-head attention: softmax(Q @ K^T / sqrt(d)) @ V\"\"\"\n",
    "    d = q.shape[-1]\n",
    "    scale = 1.0 / jnp.sqrt(d)\n",
    "    logits = jnp.einsum('bhqd,bhkd->bhqk', q, k) * scale\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    o = jnp.einsum('bhqk,bhkd->bhqd', probs, v)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Flash Attention Algorithm\n",
    "\n",
    "- Key insight: we never need the full attention matrix\n",
    "- Combine online softmax with output accumulation\n",
    "- Walk through the algorithm:\n",
    "  - Tile Q (outer parallel loop)\n",
    "  - Tile K, V (inner sequential loop)\n",
    "  - Maintain running max `m`, sum `l`, and output accumulator `o`\n",
    "  - Correction factor when max changes\n",
    "  - Final normalization\n",
    "- Python reference implementation (like your `online_softmax` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Kernel\n",
    "\n",
    "- BlockSpec design: Q tiled, K/V full sequence for inner loop\n",
    "- The kernel implementation\n",
    "- Storing logsumexp for backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import math\n",
    "\n",
    "from jax.experimental import pallas as pl\n",
    "from jax.experimental.pallas import triton as plgpu\n",
    "\n",
    "INTERPRET_MODE = True  # Set to False on GPU\n",
    "\n",
    "BLOCK_R = 64  # Block size for rows (Q blocks)\n",
    "BLOCK_C = 64  # Block size for columns (KV blocks)\n",
    "NUM_WARPS = 4\n",
    "NUM_STAGES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def flash_attention_fwd_kernel(q_ref, k_ref, v_ref, o_ref, logsumexp_ref, *, scale, num_k_blocks):\n",
    "    \"\"\"Flash attention forward kernel.\"\"\"\n",
    "    q_reg = plgpu.load(q_ref.at[0, :, :]).astype(jnp.float32)\n",
    "    o_reg = jnp.zeros(q_reg.shape, jnp.float32)\n",
    "    max_reg = jnp.full((BLOCK_R,), -jnp.inf, dtype=jnp.float32)\n",
    "    l_reg = jnp.zeros((BLOCK_R,), dtype=jnp.float32)\n",
    "    logsumexp_reg = jnp.zeros((BLOCK_R,), dtype=jnp.float32)\n",
    "\n",
    "    def body(t, args):\n",
    "        max_reg, l_reg, o_reg = args\n",
    "        idx = pl.dslice(t * BLOCK_C, BLOCK_C)\n",
    "        k_blk = plgpu.load(k_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        v_blk = plgpu.load(v_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        s_blk = pl.dot(q_reg, k_blk, trans_b=True) / scale\n",
    "        max_blk = jnp.maximum(max_reg, jnp.max(s_blk, axis=-1))\n",
    "        s_blk = jnp.exp(s_blk - max_blk[:, None])\n",
    "        l_blk = jnp.sum(s_blk, axis=-1)\n",
    "        o_blk = pl.dot(s_blk, v_blk)\n",
    "        return (max_blk, \n",
    "                l_reg * jnp.exp(max_reg - max_blk) + l_blk, \n",
    "                o_reg * jnp.exp(max_reg - max_blk)[:, None] + o_blk)\n",
    "\n",
    "    max_reg, l_reg, o_reg = jax.lax.fori_loop(0, num_k_blocks, body, (max_reg, l_reg, o_reg))\n",
    "    logsumexp_reg = max_reg + jnp.log(l_reg)\n",
    "    o_reg = o_reg / l_reg[:, None]\n",
    "    plgpu.store(o_ref.at[0, :, :], o_reg.astype(o_ref.dtype))\n",
    "    plgpu.store(logsumexp_ref.at[0, :], logsumexp_reg.astype(logsumexp_ref.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def flash_attention_fwd(q, k, v):\n",
    "    \"\"\"Flash attention forward pass.\"\"\"\n",
    "    B, H, T, C = q.shape\n",
    "    B_flat = B*H\n",
    "    q_flat = q.reshape(-1, T, C)\n",
    "    k_flat = k.reshape(-1, T, C)\n",
    "    v_flat = v.reshape(-1, T, C)\n",
    "    scale = math.sqrt(C)\n",
    "    num_k_blocks = pl.cdiv(T, BLOCK_C)\n",
    "    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n",
    "\n",
    "    out_flat, logsumexp = pl.pallas_call(\n",
    "        partial(flash_attention_fwd_kernel, scale=scale, num_k_blocks=num_k_blocks),\n",
    "        out_shape=[\n",
    "            jax.ShapeDtypeStruct(q_flat.shape, q_flat.dtype),\n",
    "            jax.ShapeDtypeStruct((B*H, T), q_flat.dtype)\n",
    "        ],\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0))\n",
    "        ],\n",
    "        out_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t))\n",
    "        ],\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES\n",
    "        )\n",
    "    )(q_flat, k_flat, v_flat)\n",
    "    out = out_flat.reshape(q.shape)\n",
    "    logsumexp = logsumexp.reshape(B, H, T)\n",
    "    return out, logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass\n",
    "\n",
    "The backward pass computes gradients dQ, dK, dV given the upstream gradient dO. The key insight is that we can recompute the attention weights P from the stored logsumexp values rather than storing them:\n",
    "\n",
    "$$P = \\exp(QK^T / \\sqrt{d} - \\text{logsumexp})$$\n",
    "\n",
    "We use three separate kernels to avoid atomic operations:\n",
    "1. **Preprocess**: Compute $D = \\text{rowsum}(O \\odot dO)$ which is used in the softmax backward\n",
    "2. **dK/dV kernel**: Outer loop over KV blocks, inner loop over Q blocks\n",
    "3. **dQ kernel**: Outer loop over Q blocks, inner loop over KV blocks\n",
    "\n",
    "The gradient formulas are:\n",
    "- $dP = dO \\cdot V^T$\n",
    "- $dS = P \\odot (dP - D) / \\sqrt{d}$ (softmax backward with scaling)\n",
    "- $dQ = dS \\cdot K$\n",
    "- $dK = dS^T \\cdot Q$  \n",
    "- $dV = P^T \\cdot dO$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Kernel 1: Preprocess - compute D = rowsum(O * dO)\n",
    "def flash_attention_bwd_preprocess_kernel(o_ref, do_ref, d_ref):\n",
    "    \"\"\"Compute D = rowsum(O * dO) for backward pass.\"\"\"\n",
    "    o_reg = plgpu.load(o_ref).astype(jnp.float32)\n",
    "    do_reg = plgpu.load(do_ref).astype(jnp.float32)\n",
    "    d_reg = jnp.sum(o_reg * do_reg, axis=-1)\n",
    "    plgpu.store(d_ref, d_reg.astype(d_ref.dtype))\n",
    "\n",
    "\n",
    "def flash_attention_bwd_preprocess(o_flat, do_flat):\n",
    "    \"\"\"Preprocess for backward: compute D = rowsum(O * dO).\"\"\"\n",
    "    B_flat, T, C = o_flat.shape\n",
    "    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n",
    "\n",
    "    d_flat = pl.pallas_call(\n",
    "        flash_attention_bwd_preprocess_kernel,\n",
    "        out_shape=jax.ShapeDtypeStruct((B_flat, T), jnp.float32),\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n",
    "    )(o_flat, do_flat)\n",
    "    return d_flat\n",
    "\n",
    "\n",
    "# Kernel 2: dK/dV - outer loop over KV blocks, inner loop over Q blocks\n",
    "def flash_attention_bwd_dkv_kernel(\n",
    "    q_ref, k_ref, v_ref, do_ref, logsumexp_ref, d_ref,\n",
    "    dk_ref, dv_ref,\n",
    "    *, scale, num_q_blocks\n",
    "):\n",
    "    \"\"\"Compute dK and dV gradients.\"\"\"\n",
    "    k_reg = plgpu.load(k_ref.at[0, :, :]).astype(jnp.float32)\n",
    "    v_reg = plgpu.load(v_ref.at[0, :, :]).astype(jnp.float32)\n",
    "\n",
    "    dk_acc = jnp.zeros(dk_ref.shape, dtype=jnp.float32)\n",
    "    dv_acc = jnp.zeros(dv_ref.shape, dtype=jnp.float32)\n",
    "\n",
    "    def body(t, carry):\n",
    "        dk_acc, dv_acc = carry\n",
    "        idx = pl.dslice(t * BLOCK_R, BLOCK_R)\n",
    "        q_blk = plgpu.load(q_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        do_blk = plgpu.load(do_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        logsumexp_blk = plgpu.load(logsumexp_ref.at[0, idx]).astype(jnp.float32)\n",
    "        d_blk = plgpu.load(d_ref.at[0, idx]).astype(jnp.float32)\n",
    "        # Recompute P = softmax(Q @ K^T / scale)\n",
    "        s_blk = pl.dot(q_blk, k_reg, trans_b=True) / scale\n",
    "        p_blk = jnp.exp(s_blk - logsumexp_blk[..., None])\n",
    "        # dP = dO @ V^T, dS = P * (dP - D) / scale\n",
    "        dp_blk = pl.dot(do_blk, v_reg, trans_b=True)\n",
    "        ds_blk = p_blk * (dp_blk - d_blk[..., None]) / scale\n",
    "        # Accumulate: dV += P^T @ dO, dK += dS^T @ Q\n",
    "        dv_acc += pl.dot(p_blk, do_blk, trans_a=True)\n",
    "        dk_acc += pl.dot(ds_blk, q_blk, trans_a=True)\n",
    "        return dk_acc, dv_acc\n",
    "        \n",
    "    dk_acc, dv_acc = jax.lax.fori_loop(0, num_q_blocks, body, (dk_acc, dv_acc))\n",
    "    plgpu.store(dk_ref, dk_acc.astype(dk_ref.dtype))\n",
    "    plgpu.store(dv_ref, dv_acc.astype(dv_ref.dtype))\n",
    "\n",
    "\n",
    "def flash_attention_bwd_dkv(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale):\n",
    "    \"\"\"Compute dK and dV using pallas_call.\"\"\"\n",
    "    B_flat, T, C = q_flat.shape\n",
    "    num_q_blocks = pl.cdiv(T, BLOCK_R)\n",
    "    grid = (B_flat, pl.cdiv(T, BLOCK_C))\n",
    "\n",
    "    dk_flat, dv_flat = pl.pallas_call(\n",
    "        partial(flash_attention_bwd_dkv_kernel, scale=scale, num_q_blocks=num_q_blocks),\n",
    "        out_shape=[\n",
    "            jax.ShapeDtypeStruct(k_flat.shape, k_flat.dtype),\n",
    "            jax.ShapeDtypeStruct(v_flat.shape, v_flat.dtype),\n",
    "        ],\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # q (full)\n",
    "            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)), # k (blocked)\n",
    "            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)), # v (blocked)\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # do (full)\n",
    "            pl.BlockSpec((1, T), lambda b, _: (b, 0)),             # logsumexp (full)\n",
    "            pl.BlockSpec((1, T), lambda b, _: (b, 0)),             # d (full)\n",
    "        ],\n",
    "        out_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)),\n",
    "            pl.BlockSpec((1, BLOCK_C, C), lambda b, t: (b, t, 0)),\n",
    "        ],\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n",
    "    )(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat)\n",
    "    return dk_flat, dv_flat\n",
    "\n",
    "\n",
    "# Kernel 3: dQ - outer loop over Q blocks, inner loop over KV blocks\n",
    "def flash_attention_bwd_dq_kernel(\n",
    "    q_ref, k_ref, v_ref, do_ref, logsumexp_ref, d_ref,\n",
    "    dq_ref,\n",
    "    *, scale, num_kv_blocks\n",
    "):\n",
    "    \"\"\"Compute dQ gradient.\"\"\"\n",
    "    q_reg = plgpu.load(q_ref.at[0, :, :]).astype(jnp.float32)\n",
    "    do_reg = plgpu.load(do_ref.at[0, :, :]).astype(jnp.float32)\n",
    "    logsumexp_reg = plgpu.load(logsumexp_ref.at[0, :]).astype(jnp.float32)\n",
    "    d_reg = plgpu.load(d_ref.at[0, :]).astype(jnp.float32)\n",
    "    dq_acc = jnp.zeros(dq_ref.shape, dtype=jnp.float32)\n",
    "\n",
    "    def body(t, carry):\n",
    "        dq_acc = carry\n",
    "        idx = pl.dslice(t * BLOCK_C, BLOCK_C)\n",
    "        k_blk = plgpu.load(k_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        v_blk = plgpu.load(v_ref.at[0, idx, :]).astype(jnp.float32)\n",
    "        # Recompute P\n",
    "        s_blk = pl.dot(q_reg, k_blk, trans_b=True) / scale\n",
    "        p_blk = jnp.exp(s_blk - logsumexp_reg[..., None])\n",
    "        # dP = dO @ V^T, dS = P * (dP - D) / scale\n",
    "        dp_blk = pl.dot(do_reg, v_blk, trans_b=True)\n",
    "        ds_blk = p_blk * (dp_blk - d_reg[..., None]) / scale\n",
    "        # Accumulate: dQ += dS @ K\n",
    "        dq_acc += pl.dot(ds_blk, k_blk)\n",
    "        return dq_acc\n",
    "\n",
    "    dq_acc = jax.lax.fori_loop(0, num_kv_blocks, body, dq_acc)\n",
    "    plgpu.store(dq_ref, dq_acc.astype(dq_ref.dtype))\n",
    "\n",
    "\n",
    "def flash_attention_bwd_dq(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale):\n",
    "    \"\"\"Compute dQ using pallas_call.\"\"\"\n",
    "    B_flat, T, C = q_flat.shape\n",
    "    num_kv_blocks = pl.cdiv(T, BLOCK_C)\n",
    "    grid = (B_flat, pl.cdiv(T, BLOCK_R))\n",
    "\n",
    "    dq_flat = pl.pallas_call(\n",
    "        partial(flash_attention_bwd_dq_kernel, scale=scale, num_kv_blocks=num_kv_blocks),\n",
    "        out_shape=jax.ShapeDtypeStruct(q_flat.shape, q_flat.dtype),\n",
    "        grid=grid,\n",
    "        in_specs=[\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)), # q (blocked)\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # k (full)\n",
    "            pl.BlockSpec((1, T, C), lambda b, _: (b, 0, 0)),       # v (full)\n",
    "            pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)), # do (blocked)\n",
    "            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),       # logsumexp (blocked)\n",
    "            pl.BlockSpec((1, BLOCK_R), lambda b, t: (b, t)),       # d (blocked)\n",
    "        ],\n",
    "        out_specs=pl.BlockSpec((1, BLOCK_R, C), lambda b, t: (b, t, 0)),\n",
    "        interpret=INTERPRET_MODE,\n",
    "        compiler_params=plgpu.CompilerParams(num_warps=NUM_WARPS, num_stages=NUM_STAGES)\n",
    "    )(q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat)\n",
    "    return dq_flat\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def flash_attention_bwd(q, k, v, o, logsumexp, do):\n",
    "    \"\"\"Flash attention backward pass using 3 separate kernels.\"\"\"\n",
    "    B, H, T, C = q.shape\n",
    "    scale = math.sqrt(C)\n",
    "\n",
    "    # Flatten batch and head dimensions\n",
    "    q_flat = q.reshape(-1, T, C)\n",
    "    k_flat = k.reshape(-1, T, C)\n",
    "    v_flat = v.reshape(-1, T, C)\n",
    "    o_flat = o.reshape(-1, T, C)\n",
    "    do_flat = do.reshape(-1, T, C)\n",
    "    logsumexp_flat = logsumexp.reshape(-1, T)\n",
    "\n",
    "    # Kernel 1: Preprocess - compute D = rowsum(O * dO)\n",
    "    d_flat = flash_attention_bwd_preprocess(o_flat, do_flat)\n",
    "\n",
    "    # Kernel 2: Compute dK, dV\n",
    "    dk_flat, dv_flat = flash_attention_bwd_dkv(\n",
    "        q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale\n",
    "    )\n",
    "\n",
    "    # Kernel 3: Compute dQ\n",
    "    dq_flat = flash_attention_bwd_dq(\n",
    "        q_flat, k_flat, v_flat, do_flat, logsumexp_flat, d_flat, scale\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        dq_flat.reshape(q.shape),\n",
    "        dk_flat.reshape(k.shape),\n",
    "        dv_flat.reshape(v.shape),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom VJP Integration\n",
    "\n",
    "- Wiring up forward and backward with `jax.custom_vjp`\n",
    "- The residuals needed (Q, K, V, O, logsumexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "@jax.custom_vjp\n",
    "def flash_attention(q, k, v):\n",
    "    \"\"\"Flash attention with custom backward pass.\"\"\"\n",
    "    o, _ = flash_attention_fwd(q, k, v)\n",
    "    return o\n",
    "\n",
    "\n",
    "def flash_attention_fwd_rule(q, k, v):\n",
    "    \"\"\"Forward rule for custom_vjp.\n",
    "    \n",
    "    Returns the output and residuals needed for backward pass.\n",
    "    \"\"\"\n",
    "    o, logsumexp = flash_attention_fwd(q, k, v)\n",
    "    return o, (q, k, v, o, logsumexp)\n",
    "\n",
    "\n",
    "def flash_attention_bwd_rule(res, do):\n",
    "    \"\"\"Backward rule for custom_vjp.\n",
    "    \n",
    "    Takes residuals from forward and upstream gradient dO,\n",
    "    returns gradients (dQ, dK, dV).\n",
    "    \"\"\"\n",
    "    q, k, v, o, logsumexp = res\n",
    "    dq, dk, dv = flash_attention_bwd(q, k, v, o, logsumexp, do)\n",
    "    return dq, dk, dv\n",
    "\n",
    "\n",
    "flash_attention.defvjp(flash_attention_fwd_rule, flash_attention_bwd_rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- Gradient correctness check against JAX autodiff\n",
    "- Train a small transformer or attention layer to verify end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference output shape: (2, 4, 256, 64)\n",
      "Flash attention output shape: (2, 4, 256, 64)\n",
      "Forward pass matches: True\n"
     ]
    }
   ],
   "source": [
    "B, H, T, D = 2, 4, 256, 64\n",
    "key = jax.random.key(0)\n",
    "keys = jax.random.split(key, 4)\n",
    "\n",
    "q = jax.random.normal(keys[0], (B, H, T, D), dtype=jnp.float32)\n",
    "k = jax.random.normal(keys[1], (B, H, T, D), dtype=jnp.float32)\n",
    "v = jax.random.normal(keys[2], (B, H, T, D), dtype=jnp.float32)\n",
    "do = jax.random.normal(keys[3], (B, H, T, D), dtype=jnp.float32)\n",
    "\n",
    "# Forward check\n",
    "o_ref = mha_reference(q, k, v)\n",
    "print(f\"Reference output shape: {o_ref.shape}\")\n",
    "\n",
    "o_flash = flash_attention(q, k, v)\n",
    "print(f\"Flash attention output shape: {o_flash.shape}\")\n",
    "print(f\"Forward pass matches: {jnp.allclose(o_flash, o_ref, atol=1e-2, rtol=1e-2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference gradient shapes: dq=(2, 4, 256, 64), dk=(2, 4, 256, 64), dv=(2, 4, 256, 64)\n",
      "Flash attention gradient shapes: dq=(2, 4, 256, 64), dk=(2, 4, 256, 64), dv=(2, 4, 256, 64)\n",
      "dQ matches: True\n",
      "dK matches: True\n",
      "dV matches: True\n",
      "\n",
      "Max differences:\n",
      "  dQ: 0.000000\n",
      "  dK: 0.000000\n",
      "  dV: 0.000001\n"
     ]
    }
   ],
   "source": [
    "# Backward check (reference)\n",
    "def loss_ref(q, k, v):\n",
    "    return jnp.sum(mha_reference(q, k, v) * do)\n",
    "\n",
    "dq_ref, dk_ref, dv_ref = jax.grad(loss_ref, argnums=(0, 1, 2))(q, k, v)\n",
    "print(f\"Reference gradient shapes: dq={dq_ref.shape}, dk={dk_ref.shape}, dv={dv_ref.shape}\")\n",
    "\n",
    "# Flash attention backward pass\n",
    "def loss_flash(q, k, v):\n",
    "    return jnp.sum(flash_attention(q, k, v) * do)\n",
    "\n",
    "dq_flash, dk_flash, dv_flash = jax.grad(loss_flash, argnums=(0, 1, 2))(q, k, v)\n",
    "print(f\"Flash attention gradient shapes: dq={dq_flash.shape}, dk={dk_flash.shape}, dv={dv_flash.shape}\")\n",
    "\n",
    "print(f\"dQ matches: {jnp.allclose(dq_flash, dq_ref, atol=1e-2, rtol=1e-2)}\")\n",
    "print(f\"dK matches: {jnp.allclose(dk_flash, dk_ref, atol=1e-2, rtol=1e-2)}\")\n",
    "print(f\"dV matches: {jnp.allclose(dv_flash, dv_ref, atol=1e-2, rtol=1e-2)}\")\n",
    "\n",
    "# Print max differences for debugging\n",
    "print(f\"\\nMax differences:\")\n",
    "print(f\"  dQ: {jnp.max(jnp.abs(dq_flash - dq_ref)):.6f}\")\n",
    "print(f\"  dK: {jnp.max(jnp.abs(dk_flash - dk_ref)):.6f}\")\n",
    "print(f\"  dV: {jnp.max(jnp.abs(dv_flash - dv_ref)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Performance Comparison\n\nWe compare our Pallas flash attention implementation against:\n1. **JAX cuDNN**: `jax.nn.dot_product_attention(implementation='cudnn')` - NVIDIA's highly optimized implementation\n2. **Reference (materialized)**: Standard attention that materializes the full N×N attention matrix\n\nNote: The cuDNN implementation requires a GPU with cuDNN installed and uses float16 for optimal performance. Set `INTERPRET_MODE = False` to run on GPU.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\ndef bench(fn, *args, iters=10):\n    for _ in range(3):  # warmup\n        result = fn(*args)\n        if isinstance(result, tuple):\n            result[0].block_until_ready()\n        else:\n            result.block_until_ready()\n    times = []\n    for _ in range(iters):\n        t0 = time.perf_counter()\n        result = fn(*args)\n        if isinstance(result, tuple):\n            result[0].block_until_ready()\n        else:\n            result.block_until_ready()\n        times.append(time.perf_counter() - t0)\n    return sum(times) / len(times)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Performance benchmark (requires GPU with cuDNN)\n# Skip this cell if running on CPU\n\ndef benchmark_attention():\n    \"\"\"Benchmark attention implementations.\"\"\"\n    import time\n    \n    # Use float16 for cuDNN compatibility\n    B, H, T, D = 4, 8, 1024, 64\n    key = jax.random.key(42)\n    keys = jax.random.split(key, 4)\n    \n    q = jax.random.normal(keys[0], (B, H, T, D), dtype=jnp.float16)\n    k = jax.random.normal(keys[1], (B, H, T, D), dtype=jnp.float16)\n    v = jax.random.normal(keys[2], (B, H, T, D), dtype=jnp.float16)\n    do = jax.random.normal(keys[3], (B, H, T, D), dtype=jnp.float16)\n    \n    print(f\"Benchmark shape: B={B}, H={H}, T={T}, D={D}, dtype=float16\")\n    print(\"=\" * 60)\n    \n    def bench_fwd(fn, q, k, v, iters=20):\n        # Warmup\n        for _ in range(3):\n            out = fn(q, k, v)\n            jax.block_until_ready(out)\n        # Bench\n        times = []\n        for _ in range(iters):\n            t0 = time.perf_counter()\n            out = fn(q, k, v)\n            jax.block_until_ready(out)\n            times.append(time.perf_counter() - t0)\n        return sum(times) / len(times) * 1000  # ms\n\n    def bench_bwd(fn, q, k, v, do, iters=20):\n        # Warmup\n        for _ in range(3):\n            grads = jax.grad(lambda q, k, v: jnp.sum(fn(q, k, v) * do), argnums=(0, 1, 2))(q, k, v)\n            jax.block_until_ready(grads)\n        # Bench\n        times = []\n        for _ in range(iters):\n            t0 = time.perf_counter()\n            grads = jax.grad(lambda q, k, v: jnp.sum(fn(q, k, v) * do), argnums=(0, 1, 2))(q, k, v)\n            jax.block_until_ready(grads)\n            times.append(time.perf_counter() - t0)\n        return sum(times) / len(times) * 1000  # ms\n\n    # JAX cuDNN (requires GPU)\n    @jax.jit\n    def jax_cudnn_attention(q, k, v):\n        # Transpose from (B, H, T, D) to (B, T, H, D) for jax.nn.dot_product_attention\n        q_t = jnp.transpose(q, (0, 2, 1, 3))\n        k_t = jnp.transpose(k, (0, 2, 1, 3))\n        v_t = jnp.transpose(v, (0, 2, 1, 3))\n        out = jax.nn.dot_product_attention(q_t, k_t, v_t, implementation='cudnn')\n        return jnp.transpose(out, (0, 2, 1, 3))\n\n    # Our Pallas implementation\n    @jax.jit\n    def pallas_attention(q, k, v):\n        return flash_attention(q, k, v)\n\n    # Reference (materialized attention matrix)\n    @jax.jit \n    def reference_attention(q, k, v):\n        return mha_reference(q, k, v)\n\n    print(\"\\nForward pass:\")\n    try:\n        t_cudnn = bench_fwd(jax_cudnn_attention, q, k, v)\n        print(f\"  JAX cuDNN:              {t_cudnn:.3f} ms\")\n    except Exception as e:\n        print(f\"  JAX cuDNN:              N/A (cuDNN not available)\")\n        t_cudnn = None\n    \n    t_pallas = bench_fwd(pallas_attention, q, k, v)\n    print(f\"  Our Pallas:             {t_pallas:.3f} ms\")\n    \n    t_ref = bench_fwd(reference_attention, q, k, v)\n    print(f\"  Reference (materialized): {t_ref:.3f} ms\")\n    \n    if t_cudnn:\n        print(f\"\\n  Pallas vs cuDNN: {t_pallas/t_cudnn:.2f}x slower\")\n\n    print(\"\\nBackward pass:\")\n    try:\n        t_cudnn_bwd = bench_bwd(jax_cudnn_attention, q, k, v, do)\n        print(f\"  JAX cuDNN:              {t_cudnn_bwd:.3f} ms\")\n    except Exception as e:\n        print(f\"  JAX cuDNN:              N/A (cuDNN not available)\")\n        t_cudnn_bwd = None\n    \n    t_pallas_bwd = bench_bwd(pallas_attention, q, k, v, do)\n    print(f\"  Our Pallas:             {t_pallas_bwd:.3f} ms\")\n    \n    t_ref_bwd = bench_bwd(reference_attention, q, k, v, do)\n    print(f\"  Reference (materialized): {t_ref_bwd:.3f} ms\")\n    \n    if t_cudnn_bwd:\n        print(f\"\\n  Pallas vs cuDNN: {t_pallas_bwd/t_cudnn_bwd:.2f}x slower\")\n\n# Uncomment to run benchmark (requires GPU):\n# benchmark_attention()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example Results (RTX 4000 Ada)\n\nWhen run on an NVIDIA RTX 4000 Ada GPU, typical results are:\n\n```\nBenchmark shape: B=4, H=8, T=1024, D=64, dtype=float16\n============================================================\n\nForward pass:\n  JAX cuDNN:                0.368 ms\n  Our Pallas:               0.433 ms\n  Reference (materialized): 1.647 ms\n\n  Pallas vs cuDNN: 1.18x slower\n\nBackward pass:\n  JAX cuDNN:                3.230 ms\n  Our Pallas:               5.728 ms\n  Reference (materialized): 6.339 ms\n\n  Pallas vs cuDNN: 1.77x slower\n```\n\n**Key observations:**\n- Our forward pass is ~18% slower than cuDNN\n- Our backward pass is ~77% slower than cuDNN (due to 3 separate kernel launches)\n- Both are significantly faster than materializing the full attention matrix",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've implemented a complete Flash Attention kernel in JAX Pallas with both forward and backward passes. The key ideas are:\n",
    "\n",
    "1. **Online softmax**: Computing softmax in tiles without materializing the full N×N attention matrix\n",
    "2. **Correction factors**: Rescaling partial results when the running maximum changes\n",
    "3. **Recomputation**: Storing only logsumexp and recomputing attention weights in the backward pass\n",
    "4. **Three backward kernels**: Separate passes for D (preprocess), dK/dV, and dQ to avoid atomic operations\n",
    "\n",
    "The implementation achieves correctness and demonstrates the core Flash Attention algorithm clearly. While it doesn't match cuDNN performance, it serves as an excellent educational resource for understanding how memory-efficient attention works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Future Work\n",
    "\n",
    "### Performance Gap\n",
    "\n",
    "Our Pallas implementation achieves correctness but runs approximately 1.5-2x slower than NVIDIA's cuDNN flash attention on the forward pass, and the gap widens on the backward pass. The primary reasons for this performance gap are:\n",
    "\n",
    "1. **Lack of warp-level tiling**: FlashAttention-2 uses sophisticated warp-level parallelism where different warps within a thread block handle different portions of the K/V matrices. This reduces shared memory traffic and improves tensor core utilization.\n",
    "\n",
    "2. **Three separate backward kernels**: Our implementation uses three kernel launches (preprocess, dK/dV, dQ) to avoid atomic operations. Production implementations fuse these more aggressively with careful synchronization.\n",
    "\n",
    "3. **No causal masking optimization**: Causal attention can skip computation for masked positions, but our implementation computes the full attention matrix.\n",
    "\n",
    "### Pallas Limitations\n",
    "\n",
    "Pallas provides a high-level abstraction for writing GPU kernels, but it doesn't expose certain low-level primitives needed for peak performance:\n",
    "\n",
    "- **No warp-level programming**: Pallas doesn't provide access to `warp_id` or warp shuffle operations (`__shfl_sync`). You can configure `num_warps` but cannot coordinate work between warps within a block.\n",
    "\n",
    "- **Limited shared memory control**: Pallas manages shared memory implicitly through `BlockSpec`. You cannot explicitly allocate shared memory or control synchronization barriers.\n",
    "\n",
    "- **No atomic operations**: Pallas on GPU doesn't expose `atomic_add` or similar primitives, requiring separate kernels for reductions.\n",
    "\n",
    "### Path to Better Performance\n",
    "\n",
    "To close the gap with cuDNN, you would need to:\n",
    "\n",
    "1. **Switch to Triton**: Triton provides more control over memory access patterns, explicit masking with `tl.where`, and better autotuning. However, even Triton abstracts away some warp-level primitives.\n",
    "\n",
    "2. **Use CUDA C++**: For full control over warp-level tiling, shared memory, and synchronization, CUDA C++ remains necessary. This is what cuDNN and the original FlashAttention implementations use.\n",
    "\n",
    "3. **Just use the built-in**: For production workloads, `jax.nn.dot_product_attention(implementation='cudnn')` is the pragmatic choice. It's highly optimized and well-tested.\n",
    "\n",
    "### Educational Value\n",
    "\n",
    "Despite the performance gap, this Pallas implementation has significant educational value:\n",
    "\n",
    "- **Algorithm clarity**: The tiled computation with online softmax correction is clearly visible in the code\n",
    "- **Gradient derivation**: The backward pass shows exactly how gradients flow through attention\n",
    "- **Pallas patterns**: Demonstrates `BlockSpec`, `fori_loop`, and `custom_vjp` integration\n",
    "- **Debugging**: `INTERPRET_MODE=True` allows stepping through the algorithm on CPU\n",
    "\n",
    "For learning how flash attention works, this implementation is arguably better than optimized CUDA code where the algorithm is obscured by performance tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Dao, T., Fu, D., Ermon, S., Rudra, A., & Re, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. *NeurIPS 2022*. https://arxiv.org/abs/2205.14135\n",
    "2. Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. *arXiv preprint arXiv:2307.08691*. https://arxiv.org/abs/2307.08691\n",
    "3. JAX Official Flash Attention (TPU): https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n",
    "4. JAX Official Fused Attention (GPU): https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/gpu/attention.py\n",
    "5. Umar Jamil's Triton Flash Attention: https://github.com/hkproj/triton-flash-attention\n",
    "6. Sebastian Raschka - Understanding and Coding Self-Attention from Scratch: https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}