{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "progress",
   "metadata": {},
   "source": [
    "## Progress Report & Next Steps\n",
    "\n",
    "**Last Updated:** January 29, 2025\n",
    "\n",
    "### Progress Made\n",
    "- ✅ Notebook structure created with 31 cells\n",
    "- ✅ All three model classes implemented:\n",
    "  - `VGGNet_BN` - Baseline with standard BatchNorm\n",
    "  - `VGGNet_NoisyBN` - Noisy version with forced ICS\n",
    "  - `VGGNet_NoBN` - Control without BatchNorm\n",
    "- ✅ Fixed PRNG key issue (Option 1 implementation):\n",
    "  - `NoisyVGGBlock` uses `master_noise_key` split during forward pass\n",
    "  - Noise only applied during training, disabled during evaluation\n",
    "- ✅ Training loop with all three models simultaneously\n",
    "- ✅ Real-time visualization (loss, train/test accuracy)\n",
    "- ✅ Results visualization and analysis code\n",
    "- ✅ Automated hypothesis testing\n",
    "- ✅ Validation and debugging checks\n",
    "\n",
    "### Current Configuration\n",
    "- Dataset: CIFAR-10 (no augmentation)\n",
    "- Batch size: 128\n",
    "- Architecture: VGG-style (10 conv + 3 FC layers)\n",
    "- Learning rate: 0.035\n",
    "- Momentum: 0.9\n",
    "- Noise parameters: shift=1.0, scale=0.2\n",
    "- Training epochs: 39\n",
    "- Random seeds: 1337 (models), 1338 (noise)\n",
    "\n",
    "### Status\n",
    "- Notebook is complete and ready to execute\n",
    "- All cells are syntactically valid\n",
    "- No known bugs blocking execution\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Phase 1: Initial Validation (DO THIS FIRST)**\n",
    "1. Run dataset loading cell - verify CIFAR-10 loads correctly\n",
    "2. Run model initialization cell - verify all three models initialize\n",
    "3. Run validation cell - check output shapes and noise behavior\n",
    "4. **STOP and verify all checks pass before proceeding**\n",
    "\n",
    "**Phase 2: Short Training Run**\n",
    "1. Modify `num_epochs = 3` (in training setup cell)\n",
    "2. Run training loop - verify:\n",
    "   - All models train without crashes\n",
    "   - Loss decreases for all models\n",
    "   - Accuracy improves\n",
    "   - Visualizations work correctly\n",
    "3. Check that noisy_bn produces different outputs on each call (stochastic)\n",
    "4. Stop and analyze short results\n",
    "\n",
    "**Phase 3: Full Experiment Execution**\n",
    "1. Restore `num_epochs = 39`\n",
    "2. Run full training loop (~6-8 hours expected)\n",
    "3. Monitor for issues:\n",
    "   - NaN losses or diverging\n",
    "   - Out-of-memory errors\n",
    "   - Training instability\n",
    "   - Unexpected long runtimes\n",
    "4. Wait for completion or interrupt early if needed\n",
    "\n",
    "**Phase 4: Results Analysis**\n",
    "1. Review final visualizations\n",
    "2. Check if hypothesis is supported (|Baseline BN - Noisy BN| < 2%)\n",
    "3. Document findings in analysis output\n",
    "4. Consider next experiments if needed\n",
    "\n",
    "### Expected Outcomes\n",
    "- **If Santurkar et al hypothesis is correct:**\n",
    "  - |Baseline BN - Noisy BN| < 2%\n",
    "  - Both BN models >> Non-BN\n",
    "  - This suggests BN effectiveness is independent of ICS reduction\n",
    "\n",
    "- **If hypothesis is NOT supported:**\n",
    "  - |Baseline BN - Noisy BN| > 2%\n",
    "  - May indicate issues with noise implementation\n",
    "  - May suggest ICS reduction plays a role\n",
    "\n",
    "### Known Issues to Watch For\n",
    "- Memory usage: 3 models simultaneously may be memory-intensive\n",
    "- Training time: ~6-8 hours, be prepared to wait or run overnight\n",
    "- Noise parameters: shift=1.0, scale=0.2 might need adjustment\n",
    "- PRNG key state: Ensure keys are properly managed during long training\n",
    "\n",
    "---\n",
    "**Ready to proceed with Phase 1 (Initial Validation)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "references",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Papers\n",
    "\n",
    "1. **Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018).** [How does batch normalization help optimization?](https://arxiv.org/abs/1805.11604). NeurIPS 31.\n",
    "   - Key finding: BN's effectiveness is independent of Internal Covariate Shift (ICS) reduction\n",
    "   - Proposed mechanism: Loss landscape smoothing, gradient predictiveness, better initialization\n",
    "\n",
    "2. **Ioffe, S., & Szegedy, C. (2015).** [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167). ICML 2015.\n",
    "   - Original ICS theory: BN reduces distributional shift in layer activations\n",
    "   - This notebook tests Santurkar's claim that ICS reduction is not the causal mechanism\n",
    "\n",
    "3. **Duchi, J., Hazan, E., Singer, Y., & Chandra, T. (2011).** [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](https://jmlr.org/papers/v12/duchi11a.html). JML 12(7):2121-2159.\n",
    "   - Original Adagrad optimizer with adaptive learning rates based on gradient history\n",
    "   - AdaGrad uses diagonal preconditioning - each dimension gets its own learning rate\n",
    "\n",
    "4. **Zeiler, M. D. (2012).** [ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701). Technical report.\n",
    "   - Introduced Adagrad with root mean squared propagation (RMSProp)\n",
    "   - Uses exponential moving average instead of cumulative sum, solving AdaGrad's vanishing gradient problem\n",
    "\n",
    "5. **Kingma, D. P., & Ba, J. (2015).** [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). ICLR (2).\n",
    "   - Combines momentum (RMSProp) with adaptive learning rates\n",
    "   - Most widely used optimizer for deep learning\n",
    "\n",
    "6. **Loshchilov, I., & Hutter, F. (2017).** [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101). ICLR (3).\n",
    "   - AdamW separates weight decay from adaptive learning rate for better regularization\n",
    "   - Addresses Adam's weight decay issues\n",
    "\n",
    "### Blog Posts & Tutorials\n",
    "\n",
    "1. [How Does Batch Normalization Work? Part 1](https://blog.vikrampawar.com/how-batchnorm-works.html) - Initial BN introduction and Ioffe's ICS theory\n",
    "2. [How Does Batch Normalization Work? Part 2](https://blog.vikrampawar.com/how-batchnorm-works-part-2.html) - Ioffe vs Santurkar comparison and landscape measurements\n",
    "\n",
    "### Optimization Theory\n",
    "\n",
    "1. [Preconditioning in Optimization](https://en.wikipedia.org/wiki/Preconditioning) - Mathematical foundation for rescaling the optimization problem\n",
    "2. [Lipschitz Continuity](https://en.wikipedia.org/wiki/Lipschitz_continuity) - Smoothness properties of functions\n",
    "3. [Gradient Descent Algorithms](https://en.wikipedia.org/wiki/Gradient_descent#Momentum) - Background on momentum methods\n",
    "\n",
    "### Related Work\n",
    "\n",
    "1. [How Does Batch Normalization Help Optimization? - Paper Analysis](https://arxiv.org/abs/1805.11604) - Santurkar et al paper with experiments\n",
    "2. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) - Original Ioffe & Szegedy paper3. [Understanding the Failure of Batch Normalization for Transformers in NLP](https://arxiv.org/abs/2002.03388) - BN issues with sequence models\n",
    "### Implementation Resources\n",
    "\n",
    "1. [Flax Documentation](https://flax.readthedocs.io/en/latest/index.html) - Framework used for model implementation\n",
    "2. [JAX Documentation](https://jax.readthedocs.io/en/latest/index.html) - Autograd and JIT compilation library\n",
    "3. [Optax](https://optax.readthedocs.io/en/latest/index.html) - Optimization library with standard optimizers\n",
    "\n",
    "### Tools & Libraries\n",
    "\n",
    "1. [deepkit](https://github.com/novastar53/deepkit) - Custom utilities for experiments (datasets, loggers, metrics)\n",
    "2. [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html) - Image classification dataset used in experiments3. [Matplotlib](https://matplotlib.org/) - Visualization library\n",
    "4. [Seaborn](https://seaborn.pydata.org/) - Statistical data visualization (plotting style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# BatchNorm Noise Injection Experiment\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook replicates Santurkar et al (2018) experiment demonstrating that BatchNorm's effectiveness is independent of Internal Covariate Shift (ICS) reduction.\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "BatchNorm's benefits come from **structural smoothing of loss landscape**, not from reducing distributional ICS. If this is true:\n",
    "\n",
    "- **Baseline BN** ≈ **Noisy BN** (similar performance)\n",
    "- Both BN models >> **Non-BN** (significantly better)\n",
    "\n",
    "### Experiment Design\n",
    "\n",
    "1. **Baseline BN:** Standard VGG + BatchNorm\n",
    "2. **Noisy BN:** VGG + BatchNorm with forced random shift/scale noise before BN normalization\n",
    "3. **Non-BN:** VGG without BatchNorm\n",
    "\n",
    "All models use identical:\n",
    "- Architecture (10 conv layers + 3 FC layers)\n",
    "- Initialization (same random seed)\n",
    "- Hyperparameters (lr=0.035, momentum=0.9)\n",
    "- Training conditions (39 epochs, CIFAR-10)\n",
    "\n",
    "### Expected Results (Santurkar et al)\n",
    "\n",
    "If ICS reduction is NOT causal mechanism:\n",
    "- Noisy BN should achieve similar performance to Baseline BN despite forced high ICS\n",
    "- Both BN models should significantly outperform Non-BN\n",
    "\n",
    "### Reference\n",
    "\n",
    "- Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). [How does batch normalization help optimization?](https://arxiv.org/abs/1805.11604). NeurIPS 31."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup & Imports\n",
    "\n",
    "Import all necessary libraries and configure plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "from deepkit.datasets import load_CIFAR10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "Load CIFAR-10 dataset without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dataset-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 391\n",
      "Test batches: 79\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_CIFAR10(augment=False)\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "Define three model variants:\n",
    "1. **Baseline BN:** Standard VGG block with BatchNorm\n",
    "2. **Noisy BN:** VGG block with BatchNorm + forced noise before BN\n",
    "3. **Non-BN:** VGG block without BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-bn",
   "metadata": {},
   "source": [
    "### Baseline VGG Block (Standard BN)\n",
    "\n",
    "Standard VGG block with BatchNorm after convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baseline-bn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = nnx.initializers.glorot_normal()\n",
    "class VGGBlock(nnx.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
    "        self.conv = nnx.Conv(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            kernel_size=(3, 3),\n",
    "            kernel_init=kernel_init,\n",
    "            padding='SAME',\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = nnx.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noisy-bn",
   "metadata": {},
   "source": [
    "### Noisy VGG Block (BN + Forced Noise)\n",
    "\n",
    "Apply random shift and scale noise BEFORE BatchNorm normalization to forcibly increase ICS.\n",
    "\n",
    "**Key Features:**\n",
    "- Noise only applied during training (not evaluation)\n",
    "- Per-batch random generation\n",
    "- Structured noise: shift (additive) + scale (multiplicative)\n",
    "- Parameters: `shift=1.0`, `scale=0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "noisy-bn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyVGGBlock(nnx.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        noise_shift: float = 1.0, \n",
    "        noise_scale: float = 0.2, \n",
    "        *, \n",
    "        rngs: nnx.Rngs\n",
    "    ):\n",
    "        self.conv = nnx.Conv(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            kernel_size=(3, 3), \n",
    "            kernel_init=kernel_init, \n",
    "            padding='SAME', \n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.noise_shift = noise_shift\n",
    "        self.noise_scale = noise_scale\n",
    "        self.master_noise_key = nnx.Variable(jax.random.PRNGKey(0))\n",
    "        self.bn = nnx.BatchNorm(num_features=out_features, momentum=0.90, rngs=rngs)\n",
    "        self.training = True\n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.training:\n",
    "            key, subkey = jax.random.split(self.master_noise_key.value)\n",
    "            self.master_noise_key.value = key\n",
    "            shift = jax.random.uniform(\n",
    "                subkey, x.shape, \n",
    "                minval=-self.noise_shift, \n",
    "                maxval=self.noise_shift\n",
    "            )\n",
    "            scale = jax.random.uniform(\n",
    "                subkey, x.shape, \n",
    "                minval=1.0 - self.noise_scale, \n",
    "                maxval=1.0 + self.noise_scale\n",
    "            )\n",
    "            x = (x + shift) * scale\n",
    "        x = self.bn(x)\n",
    "        x = nnx.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "non-bn",
   "metadata": {},
   "source": [
    "### Non-BN VGG Block\n",
    "\n",
    "VGG block without BatchNorm (control for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "non-bn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoBNVGGBlock(nnx.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
    "        self.conv = nnx.Conv(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            kernel_size=(3, 3), \n",
    "            kernel_init=kernel_init, \n",
    "            padding='SAME', \n",
    "            rngs=rngs\n",
    "        )\n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nnx.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "full-networks",
   "metadata": {},
   "source": [
    "### Full VGG Network Architectures\n",
    "\n",
    "Three complete network architectures with identical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "full-networks-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline BN Network\n",
    "class VGGNet_BN(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.convs = [\n",
    "            VGGBlock(in_features=3, out_features=64, rngs=rngs), \n",
    "            VGGBlock(in_features=64, out_features=64, rngs=rngs), \n",
    "            VGGBlock(in_features=64, out_features=128, rngs=rngs), \n",
    "            VGGBlock(in_features=128, out_features=128, rngs=rngs), \n",
    "            VGGBlock(in_features=128, out_features=256, rngs=rngs), \n",
    "            VGGBlock(in_features=256, out_features=256, rngs=rngs), \n",
    "            VGGBlock(in_features=256, out_features=512, rngs=rngs), \n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            VGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "        ]\n",
    "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.out = nnx.Linear(in_features=96, out_features=10, kernel_init=kernel_init, rngs=rngs) \n",
    "    def __call__(self, x):\n",
    "        max_pool_after = [1, 3, 5, 7, 9]\n",
    "        for conv_idx in range(len(self.convs)):\n",
    "            x = self.convs[conv_idx](x) \n",
    "            if conv_idx in max_pool_after: \n",
    "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2)) \n",
    "        x = x.squeeze() \n",
    "        x = self.fc1(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.fc2(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.out(x) \n",
    "        return x\n",
    "\n",
    "# Noisy BN Network\n",
    "class VGGNet_NoisyBN(nnx.Module):\n",
    "    def __init__(self, noise_shift=1.0, noise_scale=0.2, *, rngs: nnx.Rngs):\n",
    "        self.convs = [\n",
    "            NoisyVGGBlock(in_features=3, out_features=64, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=64, out_features=64, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=64, out_features=128, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=128, out_features=128, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=128, out_features=256, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=256, out_features=256, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=256, out_features=512, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=512, out_features=512, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=512, out_features=512, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=512, out_features=512, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "            NoisyVGGBlock(in_features=512, out_features=512, noise_shift=noise_shift, noise_scale=noise_scale, rngs=rngs), \n",
    "        ] \n",
    "        block_rng = jax.random.PRNGKey(1338) \n",
    "        for block in self.convs: \n",
    "            block_rng, subkey = jax.random.split(block_rng) \n",
    "            block.master_noise_key.value = subkey \n",
    "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.out = nnx.Linear(in_features=96, out_features=10, kernel_init=kernel_init, rngs=rngs) \n",
    "    def __call__(self, x):\n",
    "        max_pool_after = [1, 3, 5, 7, 9]\n",
    "        for conv_idx in range(len(self.convs)): \n",
    "            x = self.convs[conv_idx](x) \n",
    "            if conv_idx in max_pool_after: \n",
    "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2)) \n",
    "        x = x.squeeze() \n",
    "        x = self.fc1(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.fc2(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.out(x) \n",
    "        return x\n",
    "\n",
    "# Non-BN Network\n",
    "class VGGNet_NoBN(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.convs = [\n",
    "            NoBNVGGBlock(in_features=3, out_features=64, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=64, out_features=64, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=64, out_features=128, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=128, out_features=128, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=128, out_features=256, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=256, out_features=256, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=256, out_features=512, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "            NoBNVGGBlock(in_features=512, out_features=512, rngs=rngs), \n",
    "        ]\n",
    "        self.fc1 = nnx.Linear(in_features=512, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.fc2 = nnx.Linear(in_features=96, out_features=96, kernel_init=kernel_init, rngs=rngs) \n",
    "        self.out = nnx.Linear(in_features=96, out_features=10, kernel_init=kernel_init, rngs=rngs) \n",
    "    def __call__(self, x):\n",
    "        max_pool_after = [1, 3, 5, 7, 9]\n",
    "        for conv_idx in range(len(self.convs)): \n",
    "            x = self.convs[conv_idx](x) \n",
    "            if conv_idx in max_pool_after: \n",
    "                x = nnx.max_pool(x, window_shape=(2, 2), strides=(2, 2)) \n",
    "        x = x.squeeze() \n",
    "        x = self.fc1(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.fc2(x) \n",
    "        x = nnx.relu(x) \n",
    "        x = self.out(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initialization",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Initialize all three models with identical seed. Verify parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "initialization-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BN: 11,831,242 parameters\n",
      "Noisy BN: 11,831,242 parameters\n",
      "Non-BN: 11,824,330 parameters\n"
     ]
    }
   ],
   "source": [
    "rng_key = jax.random.key(1337) \n",
    "rngs = nnx.Rngs(rng_key) \n",
    "baseline_bn = VGGNet_BN(rngs=nnx.Rngs(rng_key)) \n",
    "noisy_bn = VGGNet_NoisyBN(noise_shift=1.0, noise_scale=0.2, rngs=nnx.Rngs(rng_key)) \n",
    "non_bn = VGGNet_NoBN(rngs=nnx.Rngs(rng_key)) \n",
    "\n",
    "#def count_params(model):\n",
    "#    return sum(x.size for x in jax.tree_util.tree_leaves(model) \n",
    "#                #if isinstance(x, (jnp.ndarray, jax.Array)) and not x.dtype.name.startswith('key')) \n",
    "#                if isinstance(x, (jnp.ndarray, jax.Array)) and not x.dtype.name.startswith('key')) \n",
    "#\n",
    "\n",
    "def count_params(m: nnx.Module, layer_type: str | None = None) -> int:\n",
    "    def get_size(y):\n",
    "        return y.size\n",
    "    \n",
    "    def exclude_noise_keys(path, val):\n",
    "        is_param = issubclass(val.type, nnx.Param)\n",
    "        path_str = '/'.join(map(str, path))\n",
    "        return is_param and 'noise_key' not in path_str\n",
    "    \n",
    "    if layer_type is not None:\n",
    "        def _filter(path, val):\n",
    "            return issubclass(val.type, nnx.Param) and layer_type in path\n",
    "        _, params, _ = nnx.split(m, _filter, nnx.Variable)\n",
    "    else:\n",
    "        _, params, _ = nnx.split(m, exclude_noise_keys, nnx.Variable)\n",
    "    param_counts = jax.tree_util.tree_map(get_size, params)\n",
    "    total_params = jax.tree_util.tree_reduce(\n",
    "        lambda x, y: x + y, param_counts, 0\n",
    "    )\n",
    "\n",
    "    return total_params\n",
    "\n",
    "\n",
    "baseline_params = count_params(baseline_bn) \n",
    "noisy_params = count_params(noisy_bn) \n",
    "non_params = count_params(non_bn) \n",
    "\n",
    "print(f\"Baseline BN: {baseline_params:,} parameters\") \n",
    "print(f\"Noisy BN: {noisy_params:,} parameters\") \n",
    "print(f\"Non-BN: {non_params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-setup",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Configure optimizers, loss function, and training utilities (same as Part 2 for consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "training-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.035\n",
    "momentum = 0.9\n",
    "\n",
    "baseline_opt = nnx.Optimizer(baseline_bn, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
    "noisy_opt = nnx.Optimizer(noisy_bn, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
    "non_opt = nnx.Optimizer(non_bn, optax.sgd(learning_rate=lr, momentum=momentum, nesterov=False))\n",
    "\n",
    "def loss_fn(model, batch, targets):\n",
    "    logits = model(batch)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def accuracy(model, batch, labels):\n",
    "    logits = model(batch)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    acc = jnp.sum(preds == labels) / logits.shape[0]\n",
    "    return acc\n",
    "\n",
    "@nnx.jit\n",
    "def step_fn(model: nnx.Module, optimizer: nnx.Optimizer, batch: jax.Array, labels: jax.Array):\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, batch, labels)\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "def test_accuracy(model: nnx.Module, testloader):\n",
    "    acc, n = 0, 0\n",
    "    for batch, labels in testloader:\n",
    "        batch = jnp.array(batch)\n",
    "        labels = jnp.array(labels)\n",
    "        acc += accuracy(model, batch, labels)\n",
    "        n += 1\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-loop",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train all three models simultaneously and track performance metrics.\n",
    "\n",
    "**Expected Duration:** ~6-8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "training-loop-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received KeyboardInterrupt. Exiting...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 39\n",
    "print_interval = 20\n",
    "test_interval = 200\n",
    "\n",
    "i = 0\n",
    "baseline_train_losses, noisy_train_losses, non_train_losses = [], [], []\n",
    "baseline_train_accs, noisy_train_accs, non_train_accs = [], [], []\n",
    "baseline_test_accs, noisy_test_accs, non_test_accs = [], [], []\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, labels in train_loader:\n",
    "            batch = jnp.array(batch)\n",
    "            labels = jnp.array(labels)\n",
    "\n",
    "            baseline_bn.train()\n",
    "            noisy_bn.train()\n",
    "            non_bn.train()\n",
    "\n",
    "            baseline_loss = step_fn(baseline_bn, baseline_opt, batch, labels)\n",
    "            noisy_loss = step_fn(noisy_bn, noisy_opt, batch, labels)\n",
    "            non_loss = step_fn(non_bn, non_opt, batch, labels)\n",
    "\n",
    "            baseline_train_losses.append(baseline_loss)\n",
    "            noisy_train_losses.append(noisy_loss)\n",
    "            non_train_losses.append(non_loss)\n",
    "\n",
    "            baseline_acc = accuracy(baseline_bn, batch, labels)\n",
    "            noisy_acc = accuracy(noisy_bn, batch, labels)\n",
    "            non_acc = accuracy(non_bn, batch, labels)\n",
    "\n",
    "            baseline_train_accs.append(baseline_acc)\n",
    "            noisy_train_accs.append(noisy_acc)\n",
    "            non_train_accs.append(non_acc)\n",
    "\n",
    "            baseline_bn.eval()\n",
    "            noisy_bn.eval()\n",
    "            non_bn.eval()\n",
    "\n",
    "            if i % test_interval == 0:\n",
    "                baseline_test_acc = test_accuracy(baseline_bn, test_loader)\n",
    "                noisy_test_acc = test_accuracy(noisy_bn, test_loader)\n",
    "                non_test_acc = test_accuracy(non_bn, test_loader)\n",
    "\n",
    "                baseline_test_accs.append(baseline_test_acc)\n",
    "                noisy_test_accs.append(noisy_test_acc)\n",
    "                non_test_accs.append(non_test_acc)\n",
    "\n",
    "            if i % print_interval == 0:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"iter: {i}\")\n",
    "                print(f\"  Baseline BN - loss: {baseline_loss:0.4f}, test_acc: {baseline_test_acc:0.3f}\")\n",
    "                print(f\"  Noisy BN    - loss: {noisy_loss:0.4f}, test_acc: {noisy_test_acc:0.3f}\")\n",
    "                print(f\"  Non-BN      - loss: {non_loss:0.4f}, test_acc: {non_test_acc:0.3f}\")\n",
    "\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "                axes[0].plot(baseline_train_losses, alpha=0.9, label='Baseline BN')\n",
    "                axes[0].plot(noisy_train_losses, alpha=0.7, label='Noisy BN')\n",
    "                axes[0].plot(non_train_losses, alpha=0.5, label='Non-BN')\n",
    "                axes[0].set_title('Training Loss')\n",
    "                axes[0].legend()\n",
    "                axes[1].plot(baseline_train_accs, alpha=0.9, label='Baseline BN')\n",
    "                axes[1].plot(noisy_train_accs, alpha=0.7, label='Noisy BN')\n",
    "                axes[1].plot(non_train_accs, alpha=0.5, label='Non-BN')\n",
    "                axes[1].set_title('Train Accuracy')\n",
    "                axes[1].legend()\n",
    "                x = list(range(len(baseline_test_accs)))\n",
    "                x = [i * 200 for i in x]\n",
    "                axes[2].plot(x, baseline_test_accs, alpha=0.9, label='Baseline BN')\n",
    "                axes[2].plot(x, noisy_test_accs, alpha=0.7, label='Noisy BN')\n",
    "                axes[2].plot(x, non_test_accs, alpha=0.5, label='Non-BN')\n",
    "                axes[2].set_title('Test Accuracy')\n",
    "                axes[2].legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            i += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Received KeyboardInterrupt. Exiting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "After training completes, create comprehensive comparison visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes[0, 0].plot(baseline_train_losses, alpha=0.9, label='Baseline BN')\n",
    "axes[0, 0].plot(noisy_train_losses, alpha=0.7, label='Noisy BN')\n",
    "axes[0, 0].plot(non_train_losses, alpha=0.5, label='Non-BN')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 1].plot(baseline_train_accs, alpha=0.9, label='Baseline BN')\n",
    "axes[0, 1].plot(noisy_train_accs, alpha=0.7, label='Noisy BN')\n",
    "axes[0, 1].plot(non_train_accs, alpha=0.5, label='Non-BN')\n",
    "axes[0, 1].set_title('Train Accuracy Comparison')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "x = list(range(len(baseline_test_accs)))\n",
    "x = [i * 200 for i in x]\n",
    "axes[1, 0].plot(x, baseline_test_accs, 'o-', alpha=0.9, label='Baseline BN')\n",
    "axes[1, 0].plot(x, noisy_test_accs, 's-', alpha=0.7, label='Noisy BN')\n",
    "axes[1, 0].plot(x, non_test_accs, '^-', alpha=0.5, label='Non-BN')\n",
    "axes[1, 0].set_title('Test Accuracy Comparison')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "final_accuracies = [baseline_test_accs[-1], noisy_test_accs[-1], non_test_accs[-1]]\n",
    "axes[1, 1].bar(['Baseline BN', 'Noisy BN', 'Non-BN'], final_accuracies, \n",
    "                 color=['blue', 'orange', 'red'], alpha=0.7)\n",
    "axes[1, 1].set_title('Final Test Accuracy')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "for i, v in enumerate(final_accuracies):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis",
   "metadata": {},
   "source": [
    "## Analysis & Discussion\n",
    "\n",
    "Automated analysis of results with hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFinal Performance:\")\n",
    "print(f\"  Baseline BN - Test Accuracy: {baseline_test_accs[-1]:.4f}\")\n",
    "print(f\"  Noisy BN    - Test Accuracy: {noisy_test_accs[-1]:.4f}\")\n",
    "print(f\"  Non-BN      - Test Accuracy: {non_test_accs[-1]:.4f}\")\n",
    "print(\"\\nConvergence Speed (iterations to 80% test accuracy):\")\n",
    "def find_convergence(accs, target=0.8):\n",
    "    x = list(range(len(accs)))\n",
    "    x = [i * 200 for i in x]\n",
    "    for i, acc in enumerate(accs):\n",
    "        if acc >= target:\n",
    "            return x[i]\n",
    "    return None\n",
    "baseline_conv = find_convergence(baseline_test_accs)\n",
    "noisy_conv = find_convergence(noisy_test_accs)\n",
    "non_conv = find_convergence(non_test_accs)\n",
    "print(f\"  Baseline BN: {baseline_conv if baseline_conv else 'Not reached'}\")\n",
    "print(f\"  Noisy BN:    {noisy_conv if noisy_conv else 'Not reached'}\")\n",
    "print(f\"  Non-BN:      {non_conv if non_conv else 'Not reached'}\")\n",
    "print(\"\\nKey Observation:\")\n",
    "diff_baseline_noisy = abs(baseline_test_accs[-1] - noisy_test_accs[-1])\n",
    "diff_baseline_non = abs(baseline_test_accs[-1] - non_test_accs[-1])\n",
    "print(f\"  |Baseline BN - Noisy BN|: {diff_baseline_noisy:.4f}\")\n",
    "print(f\"  |Baseline BN - Non-BN|:    {diff_baseline_non:.4f}\")\n",
    "if diff_baseline_noisy < 0.02:\n",
    "    print(\"\\n✓ Result supports Santurkar et al hypothesis:\")\n",
    "    print(\"  Noisy BN performance ≈ Baseline BN performance\")\n",
    "    print(\"  This suggests BN effectiveness is independent of ICS reduction\")\n",
    "else:\n",
    "    print(\"\\n✗ Result does NOT support Santurkar et al hypothesis:\")\n",
    "    print(\"  Noisy BN performance differs significantly from Baseline BN\")\n",
    "    print(\"  This may indicate issues with noise implementation or parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation",
   "metadata": {},
   "source": [
    "## Validation & Debugging\n",
    "\n",
    "Optional validation checks before running full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batch = jnp.ones((4, 32, 32, 3))\n",
    "baseline_out = baseline_bn(dummy_batch)\n",
    "noisy_out = noisy_bn(dummy_batch)\n",
    "non_out = non_bn(dummy_batch)\n",
    "assert baseline_out.shape == (4, 10), f\"Baseline BN output shape: {baseline_out.shape}\"\n",
    "assert noisy_out.shape == (4, 10), f\"Noisy BN output shape: {noisy_out.shape}\"\n",
    "assert non_out.shape == (4, 10), f\"Non-BN output shape: {non_out.shape}\"\n",
    "print(\"✓ Output shapes match\")\n",
    "noisy_bn.train()\n",
    "noisy_out1 = noisy_bn(dummy_batch)\n",
    "noisy_out2 = noisy_bn(dummy_batch)\n",
    "assert not jnp.array_equal(noisy_out1, noisy_out2), \"Noise not stochastic\"\n",
    "print(\"✓ Noise is stochastic (different each call)\")\n",
    "noisy_bn.eval()\n",
    "noisy_out3 = noisy_bn(dummy_batch)\n",
    "noisy_out4 = noisy_bn(dummy_batch)\n",
    "assert jnp.array_equal(noisy_out3, noisy_out4), \"Noise applied during eval\"\n",
    "print(\"✓ Noise disabled during evaluation\")\n",
    "test_batch = jnp.ones((4, 32, 32, 3))\n",
    "test_labels = jnp.array([0, 1, 2, 3])\n",
    "test_loss = step_fn(baseline_bn, baseline_opt, test_batch, test_labels)\n",
    "assert isinstance(test_loss, (float, jnp.ndarray)), f\"Loss type: {type(test_loss)}\"\n",
    "print(\"✓ Training step function works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This experiment tests Santurkar et al (2018) hypothesis that BatchNorm's effectiveness is independent of Internal Covariate Shift (ICS) reduction.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "If results show **Baseline BN ≈ Noisy BN >> Non-BN**:\n",
    "- Supports Santurkar et al's theory\n",
    "- Suggests BN's benefits come from structural smoothing, not ICS reduction\n",
    "- Demonstrates BN's adaptability (γ,β compensate for forced noise)\n",
    "\n",
    "If results show **Baseline BN >> Noisy BN**:\n",
    "- Contradicts Santurkar et al's theory\n",
    "- May indicate issues with noise implementation\n",
    "- Suggests ICS reduction may play a role\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Landscape Measurements:** Add loss Lipschitzness and gradient predictiveness (as in Part 2)\n",
    "2. **ICS Quantification:** Measure both distributional and gradient-based ICS to verify noise actually increases ICS\n",
    "3. **Noise Ablation:** Test different noise levels and distributions\n",
    "4. **Additional Controls:** Compare against other regularizers (dropout, weight decay)\n",
    "\n",
    "### References\n",
    "\n",
    "- Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). [How does batch normalization help optimization?](https://arxiv.org/abs/1805.11604). NeurIPS 31.\n",
    "- Ioffe, S., & Szegedy, C. (2015). [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167). ICML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-performance-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
